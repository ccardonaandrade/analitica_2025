---
title: Analítica de los Negocios
author: Carlos Cardona Andrade
subtitle: Intro a la Regresión Lineal
execute:
  freeze: auto
  echo: true
  fig-width: 6
  fig-height: 5
format:
  revealjs: 
   theme: ../slides.scss
   header-includes: |
      <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" rel="stylesheet"/>
   slide-number: true
   show-slide-number: all
   transition: fade
   progress: true
   multiplex: false
   scrollable: false
   preview-links: false
   hide-inactive-cursor: true
   highlight-style: printing
   pause: true
---


```{r}
#| eval: true
#| echo: false
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel,gtools)

# Define pink color
red_pink <- "#e64173"

theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, 0, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)


# Set population and sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))


# Simulation

```



## Plan para hoy

1. [Correlación](#corr)

2. [Regresión Lineal Simple](#lm)

3. [Regresión Lineal Múltiple](#multiple)

# Correlación {#corr}


## US Total Gross vs Opening Gross {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

library(tidyverse)
library(readxl)
library(janitor)
hollywood <- read_excel("C:/Users/ccard/Downloads/KEL702-XLS-ENG.xls", sheet = "Exhibit 1")
hollywood <- hollywood %>%
  clean_names()
hollywood <- hollywood %>% rename(us_gross = total_u_s_gross)
hollywood <- hollywood %>% rename(non_us_gross = total_non_u_s_gross)

```

```{r}
#| echo: true
#| eval: true
#| fig-align: center
#| code-fold: true

ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
    geom_point() +
    labs(
        x = "Opening Gross (in millions)",
        y = "US Total Gross (in millions)"
    ) +
    theme_minimal()

```

## La correlación en R {.medium}

- Podemos utilizar un diagrama de dispersión para realizar un primer análisis de la relación entre dos variables

- El coeficiente de correlación (lineal) es utilizado para medir la fuerza de la asociación (lineal) entre dos variables

- La correlación entre el recaudo en US y el recaudo el primer fin de semana:

```{r}
#| echo: true
cor(hollywood$us_gross,hollywood$opening_gross)
```


## Función `corrplot` {.medium}

```{r}
#| echo: true
#| eval: false
#| fig-align: center

# RECUERDEN INSTALAR EL PAQUETE PRIMERO!!!
# install.packages("corrplot")

# Cargamos el paquete
library(corrplot)

# Creemos una base de datos temporal sólo con estas 4 variables
hollywood_sub <- hollywood %>% select( us_gross, opening_gross, non_us_gross, budget) 

# Creemos la matriz de correlaciones
corrplot(cor(hollywood_sub ),
         method = "number",
         type = "upper")

```



## Función `corrplot` {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: center

# RECUERDEN INSTALARLA PRIMERO!!!
# install.packages("corrplot)

# Cargamos el paquete
library(corrplot)

# Creemos una base de datos temporal sólo con estas 4 variables
hollywood_sub <- hollywood %>% select( us_gross, opening_gross, non_us_gross, budget) 

# Creemos la matriz de correlaciones
corrplot(cor(hollywood_sub ),
         method = "number",
         type = "upper")

```



## La correlación no es suficiente {.medium}


```{r}
#| echo: false
#| eval: true
#| fig-align: center

library(datasauRus)
library(patchwork)

dino <- datasaurus_dozen %>%
  filter(dataset == "dino")

star <- datasaurus_dozen %>%
  filter(dataset == "star")

hlines <- datasaurus_dozen %>%
  filter(dataset == "h_lines")

dino %>%
  summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
  )



```



```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-width: 7
#| fig-height: 3.5

ggplot(dino, aes(x=x, y=y))+
  geom_point(color = "darkred") + theme_minimal()
```




## La correlación no es suficiente {.medium}


```{r}
#| echo: false
#| eval: true
#| fig-align: center
star %>%
  summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
  )
```

```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-width: 7
#| fig-height: 3.5
ggplot(star, aes(x=x, y=y))+
  geom_point(color = "navy") + theme_minimal()

```


## La correlación no es suficiente {.medium}


```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-width: 7
#| fig-height: 3.5
hlines %>%
  summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
  )
```

```{r}
#| echo: false
#| eval: true
#| fig-align: center

ggplot(hlines, aes(x=x, y=y))+
  geom_point(color = "orange") + theme_minimal()

```



# Regresión Lineal Simple {#lm}

## ¿Por qué una regresión? {.medium}

### Ejemplo

El promedio en la universidad es resultado de la habilidad y las horas estudiadas. Por lo tanto, uno podría pensar en el siguiente modelo:

$$Promedio=f(H,Saber11,PCA)$$
Donde $H$ es la habilidad, $Saber11$ es el puntaje en la prueba Saber 11 y $PCA$ es el porcentaje de clases a las que el estudiante fue. Esperamos que el $Promedio$ aumente cada que alguna de esas 3 variables aumente.

:::{.fragment}

Sin embargo, no necesitamos esperar. Podemos evaluar esta hipótesis usando un **modelo de regresión!**

:::

## ¿Por qué una regresión? {.medium}

**Modelo de Regresión:**

$$Promedio_i=\beta_0 + \beta_1H_i + \beta_2Saber11_i+\beta_3PCT_i+\varepsilon_i$$
Queremos estimar/evaluar la relación 
$$Promedio=f(H,Saber11,PCA)$$

. . .

**Preguntas a responder:**

- ¿Cómo se interpretan $\beta_0$, $\beta_1$, $\beta_2$ y $\beta_3$?

- ¿Son los términos $\beta_k$ parámetros o son estimaciones muestrales?

- ¿Qué es $\varepsilon_i$?
     
## Partes esenciales de una regresión {.medium}

$$y_i=\beta_0+\beta_1x_i+e_i$$

::: columns
::: {.column width="50%"}
### $y$  {.text-orange-gold .center}

- Variable dependiente

- Variable resultado

- Básicamente lo que queremos predecir o explicar

:::

::: {.column .fragment width="50%"}

### $x$  {.text-orange-gold .center}

- Variable explicativa

- Predictor

- Variable independiente

- Lo que usamos para predecir o explicar Y
:::
:::


## Objetivos de una regresión {.medium}

Usualmente ajustamos a una línea por dos razones:

### Predicción

- Predecir el futuro

- Nos enfocamos en Y

- Netflix tratando de predecir la siguiente serie que veremos

### Explicación

- Explicar el efecto de X en Y

- Nos enfocamos en X

- Netflix evaluando el efecto de la hora del día en la selección de una serie


## ¿Cómo se estima la línea de regresión? {.medium}

1. Graficar ambas variables $y$ y $x$

2. Dibujar una recta que se aproxime a la relación observada (ojalá funcione para datos que no están en la muestra)

3. Estimar los números que componen esa recta

4. Interpretar esos números

```{r}
#| echo: false
library(tidyverse)
library(patchwork)
library(broom)
library(knitr)
galletas <- tibble(felicidad = c(0.5, 2, 1, 2.5, 3, 1.5, 2, 2.5, 2, 3),
                  galletas = 1:10)

galletas_datos <- galletas
modelo_galletas <- lm(felicidad ~ galletas, data = galletas_datos)
predict_galletas <- augment(modelo_galletas)
```



## Galletas y Felicidad {.medium}

::: {.tbl-classic .tbl-larger .center-text}


```{r}
#| echo: false
#| eval: true

galletas |> 
  knitr::kable(format = "html") |> 
  kableExtra::kable_styling(font_size = 30) # Adjust the font size as needed
```

:::

## ¿Cómo se relacionan las galletas con la felicidad? {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4



base_cookies <- ggplot(predict_galletas, aes(x = galletas, y = felicidad)) +
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

base_cookies
```




## Trazando una curva que sigue los datos {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = lm, color = "#0074D9", formula = y ~ splines::bs(x, 7), se = FALSE)
```

## Otra línea que se adapta a los datos {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = "loess", color = "#0074D9", se = FALSE)
```

## Ajuste con una línea recta (Regresión lineal) {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE)
```

## Errores de predicción {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

galletas_con_residuo <- base_cookies +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE) +
  geom_segment(aes(xend = galletas, yend = .fitted), color = "#FF851B", size = 1)

galletas_con_residuo
```

## Visualización de los residuos

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

galletas_solo_residuo <- ggplot(predict_galletas, aes(x = galletas, y = .resid)) +
  geom_hline(yintercept = 0, color = "#B10DC9", size = 1) +
  geom_point(size = 3) +
  geom_segment(aes(xend = galletas, yend = 0), color = "#FF851B", size = 1) +
  coord_cartesian(xlim = c(0, 10), ylim = c(-1.5, 1.5)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Galletas consumidas", y = "Distancia a la línea") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

galletas_solo_residuo
```

##


```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

(galletas_con_residuo + labs(title = "Galletas y Felicidad")) + 
  (galletas_solo_residuo + labs(title = "Residuos"))
```

## Pendiente de una recta {.medium}

$$
y = mx + b
$$

::: {.tbl-classic .tbl-larger .center-text}


|     |                                          |
|:---:|:----------------------------------------:|
| $y$ |                Un número                 |
| $x$ |                Un número                 |
| $m$ | La pendiente $\frac{\Delta y}{\Delta x}$ |
| $b$ |          El intercepto con $y$           |

:::

## Pendiente de una recta {.medium}

::: columns
::: {.column width="50%"}
$$
y = 2x - 1
$$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 4.8
#| fig-height: 3.5

ggplot(data = tibble(x = 0:5), aes(x = x)) +
  stat_function(fun = function(x) 2 * x - 1, 
                color = "#BF3984", size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:5) +
  scale_y_continuous(breaks = -1:9) +
  theme(panel.grid.minor = element_blank())
```

:::



::: {.column .fragment width="50%"}
$$
y = -0.5x + 6
$$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 4.8
#| fig-height: 3.5

ggplot(data = tibble(x = 0:14), aes(x = x)) +
  stat_function(fun = function(x) -0.5 * x + 6, 
                color = "#BF3984", size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:14) +
  scale_y_continuous(breaks = -1:9) +
  theme(panel.grid.minor = element_blank())
```

:::
:::

## Regresión lineal simple {.medium}

$$
y = \beta_0 + \beta_1 x + \varepsilon
$$

-   $\beta_1$: la pendiente verdadera de la relación entre $x$ y $y$

-   $\beta_0$: el intercepto verdadero de la relación entre $x$ y $y$

-   $\varepsilon$: el error

## Regresión lineal simple {.medium}

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x 
$$

-   $\hat{\beta_1}$: la pendiente estimada de la relación entre $x$ y $y$

-   $\hat{\beta_0}$: el intercepto estimada de la relación entre $x$ y $y$

-   No hay error!!!

## Modelo de Regresión {.medium}

::: columns
::: {.column width="40%"}
$$  
  \begin{aligned}
Y &= \color{#0074D9}{\text{Modelo}} + \color{#FF851B}{\text{Error}} \\
  &= \color{#0074D9}{f(X)} + \color{#FF851B}{\varepsilon}
\end{aligned}
$$
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| out-width: "100%"


galletas_con_residuo
```
:::
:::

## Residuos {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| out-width: "100%"


galletas_con_residuo

```

$$
\text{Residuo} = \text{Observado} - \text{Predicho} = y - \hat{y}
$$

## La línea de los mínimos cuadrados ordinarios (MCO) {.medium}

-   El residuo para la observación $i^{th}$ es:

$$e_i= \text{Observado} - \text{Predicho}=y_i - \hat{y_i}$$ - La **suma de los residuos al cuadrado** es:

$$e_1^2+e_2^2+e_3^2+..+e_n^2$$

-   La **línea de los mínimos cuadrados ordinarios** es la que minimiza la suma de los residuos al cuadrado:

$$ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \sum e_i^2 $$


## 

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/meme.png")

```


## MCO vs Otras líneas {.smaller}

Volvamos a la relación felicidad y galletas...

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

Para cualquier línea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

Para cualquier línea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

Para cualquier línea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

Para cualquier línea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

$\sum e_i^2$ calcula el cuadrado de los errores: errores más grandes reciben una penalización mayor

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

La línea de MCO es la combinación de $\hat{\beta_0}$ y $\hat{\beta_1}$ que minimizan $\sum e_i^2$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

## ¿Cómo gráficar la línea de regresión? {.medium}

`geom_smooth(method="lm")`es la función dentro de ggplot para gráficar la línea de regresión y su respectivo intervalo de confianza.

::: columns
::: {.column width="40%"}

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "5" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::
:::


## ¿Cómo gráficar la línea de regresión? {.medium}

La opción `se = FALSE` elimina los intervalos de confianza.

::: columns
::: {.column width="40%"}

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "5" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE) +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "4" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE) +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::
:::




## Construyendo modelos en R {.medium}

-   El  siguiente código estima y reporta los resultados de la regresión:

```{r}
#| eval: false
#| echo: true

name_of_model <- lm(y ~ x, data = DATA)

summary(name_of_model)  # Para ver los detalles del modelo
```

::: fragment

-   La función `tidy` del paquete `broom` reporta  los resultados del modelo como un data frame para graficar:

```{r}
#| eval: false
#| echo: true

library(broom)

tidy(name_of_model)
```
:::

## Modelando Galletas y Felicidad {.medium}

::: columns
::: {.column width="40%"}
$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas$$

```{r}
#| echo: true
#| eval: true
modelo_felicidad <- 
  lm(felicidad ~ galletas,
     data = galletas_datos)
```
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| label: cookies-happiness-again
#| results: hide


base_cookies +
  geom_smooth(method = "lm", color = "#0074D9") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::
:::

## Modelando Galletas y Felicidad {.medium}

La función `tidy` nos permite ver los:

  - coeficientes
  - errores estándar
  - el estadístico $t$
  - el p-value
  - los IC
  
. . .
  
Para nuestro `modelo_felicidad`:

```{r}
#| echo: true
tidy(modelo_felicidad, conf.int = TRUE)
```



## Traduciendo los resultados a matemáticas {.medium}

::: columns
::: {.column width="40%"}
```{r}
#| echo: false
tidy(modelo_felicidad, conf.int = TRUE) |> 
  select(term, estimate)
```

$$
\begin{aligned}
&\widehat{Felicidad} = \\ 
&\hat{\beta_0}+\hat{\beta_1}\times Galletas
\end{aligned}
$$

$$
\begin{aligned}
&\widehat{Felicidad} = \\ 
&1.1 + 0.16 \times Galletas
\end{aligned}
$$
:::

::: {.column width="60%"}
```{r}
#| echo: false
base_cookies +
  geom_smooth(method = "lm", color = "#0074D9") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::
:::

## Interpretación de los coeficientes {.medium}

Un incremento en una unidad de $X$ está *asociado* con un incremento (o reducción) promedio de $\beta_1$ unidades en $Y$

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas$$

$$\widehat{Felicidad} = 1.1 + 0.16 \times Galletas$$

:::{.incremental}

- En *promedio*, una galleta adicional está asociado a aumento en la felicidad de 0.16 unidades

- Si no hay consumo de galletas, esperamos que el puntaje de felicidad sea 1.1 unidades
:::


## ¿Es el intercepto importante? {.medium}

- La interpretación del intercepto es importante si en el contexto de los datos:

  1. La variable independiente puede tomar valores iguales o cercanos a cero
  
  2. La variable independiente tiene valores cercanos a cero en los datos observados
  
:::{.incremental}
  
- En caso contrario, el intercepto no tiene ninguna interpretación práctica

- Veremos más ejemplos sobre esto más adelante...
:::


## 💪 Ejercicio 2  {.medium}

Según la sabiduría popular en Hollywood, el recaudo durante el primer fin de semana es un fuerte predictor del éxito comercial de una película.

1. Grafiquen la línea de regresión entre el recaudo total en Estados Unidos y el recaudo en el primer fin de semana para evaluar esta creencia.

2. Estimen la siguiente regresión y llámenla `hollywood_model`:

$$\widehat{\text{US Total Gross}} = \hat{\beta_0} + \hat{\beta_1} \times \text{Opening Gross}$$
3. Usando la función `tidy` reporten los valores de los coeficientes.

4. ¿Cuál es la interpretación de $\hat{\beta_1}$ en este caso?¿Y de $\hat{\beta_0}$?
 



## Predicción  {.medium}

```{r}
#| echo: false
#| eval: true
hollywood_model <- lm(us_gross ~ opening_gross, data=hollywood)
```

$$\widehat{\text{US Total Gross}} = 5,108,220 + 3.12 \times \text{Opening Gross}$$


Según nuestro modelo, ¿cuál sería el recaudo en US de una película cuyo recaudo en el primer fin de semana fue de \$50,000,000?



$$  
  \begin{aligned}
\widehat{\text{US Gross}} &= 5,108,220 + 3.12 \times \text{Opening Gross} \\
  &= 5,108,220 + 3.12 \times \color{red}{50,000,000} \\
  &= 161,108,220
\end{aligned}
$$

## Predicción con R {.medium}

El comando `predict()` nos permite predecir $\widehat{\text{US Gross}}$ para uno o varios valores:



```{r}
#| echo: true
#| eval: true

# Creamos los valores para los cuales queremos predecir
valores_opening <- data.frame(opening_gross = c(20000000,40000000,50000000))


# Predice los valores con los coeficientes estimados
# en hollywood_model
predict(hollywood_model, newdata = valores_opening)

```


## Predicción  {.medium}


```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-height: 4
#| fig-width: 8

model <- lm(us_gross ~ opening_gross , data = hollywood)

# Predict the corresponding y value when x = 50
predicted_y <- predict(model, newdata = data.frame(opening_gross = 50 * 1000000))

# Plot
ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_vline(xintercept = 50, linetype = "dashed", color = "blue") +   # Dashed vertical line at x = 50
  geom_hline(yintercept = predicted_y/ 1000000, linetype = "dashed", color = "blue") + # Dashed horizontal line at predicted y
  geom_point(aes(x = 50, y = predicted_y/ 1000000), color = "red", size = 3) + # Point at (50, predicted_y)
  labs(
    x = "Opening Gross (in millions)",
    y = "US Total Gross (in millions)"
  ) +
  theme_minimal()
```





## ¿Es posible la extrapolación? {.medium}

Extrapolar es tratar de predecir Y fuera del rango de valores de X. Es posible pero no aconsejable.

```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-height: 4
#| fig-width: 8
# Predict the corresponding y value when x = 80
predicted_y <- predict(model, newdata = data.frame(opening_gross = 80 * 1000000))

# Plot
ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
    geom_point() +
    geom_smooth(method = "lm") +
    geom_vline(xintercept = 80, linetype = "dashed", color = "blue") +   # Dashed vertical line at x = 80
    geom_hline(yintercept = predicted_y/ 1000000, linetype = "dashed", color = "blue") + # Dashed horizontal line at predicted y
    geom_point(aes(x = 80, y = predicted_y/ 1000000), color = "red", size = 3) + # Point at (80, predicted_y)
    labs(
        x = "Opening Gross (in millions)",
        y = "US Total Gross (in millions)"
    ) + theme_minimal()

```


## Inferencia de los coeficientes {.medium}

Cuando trabajamos con distribuciones muestrales, la idea era que:

$$
\bar{X} \xrightarrow{\text{🤞 ojalá 🤞}} \mu
$$

:::{.fragment}

De igual manera, en el modelo de regresión queremos:

$$
\hat{\beta} \xrightarrow{\text{🤞 ojalá 🤞}} \beta
$$
:::



## Inferencia de los coeficientes {.medium}

$$\widehat{\text{US Total Gross}} = 5,108,220 + 3.12 \times \text{Opening Gross}$$

- Es $\beta_1$ diferente de cero?


::: fragment
```{r}
#| echo: true
tidy(hollywood_model, conf.int = TRUE)
```
:::


## Más pruebas de hipótesis {.medium}

$$H_0:\beta_1=0$$
$$H_1: \beta_1 \neq 0$$

:::{.fragment}

$$Z=\dfrac{3.12-0}{0.218}=14.3>Z_{\frac{\alpha}{2}}=1.96$$

- Rechazamos la $H_0$ a un nivel de significancia del 5%! 

- El p-value es 7.07e-23 (en notación científica), el cual es mucho menor a 0.05. 

:::


# Regresión Lineal Múltiple {#multiple}

## Regresión Múltiple {.medium}

No estamos limitados a una sola variable explicativa!

$$
\hat{y} = \hat{\beta_0}  + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_n} x_n 
$$

:::{.fragment}
Podríamos pensar que el verdadero modelo de felicidad es:

  $$\widehat{\text{Felicidad}}=\hat{\beta_0}+\hat{\beta_1}\times \text{Galletas}+\hat{\beta_2} \times \text{Estudiante}$$

O que para predecir el recaudo de una película se necesita el siguiente modelo:

$$
\widehat{\text{US Gross}} = \hat{\beta_0} + \hat{\beta_1} \text{Opening Gross} + \hat{\beta_2} \text{Budget} + \hat{\beta_3} \text{Sequel}
$$

:::



## ¿Cómo pensar la relación $y=f(x_1,x_2)+\varepsilon$? {.medium}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/dots.png")

```


## ¿Cómo pensar la "recta" en una regresión múltiple? {.medium}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/plane.png")

```



## Variables Categóricas vs Variables Continuas {.medium}

En los dos modelos antes explicados hay dos tipos de variables:

### Variables Categóricas

- $\text{Estudiante}$

- $\text{Sequel}$

### Variables Continuas

- $\text{Galletas}$

- $\text{Opening Gross}$



## Variables Categóricas vs Variables Continuas {.medium}

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/slider-switch-plain-80.jpg")

```


## Variables Categóricas vs Variables Continuas {.medium}

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/slider-switch-annotated-80.jpg")

```


## Ejemplo de felicidad y galletas

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

set.seed(123)

# Create a tibble with the same slope but different intercepts for professors and students, adding noise
cookies_data <- tibble(
  cookies = rep(1:7, 2),  # Galletas consumidas (1 through 7 for both groups)
  group = rep(c("Profesores", "Estudiantes"), each = 7),  # Group variable
  happiness = c(0.5 + 0.15 * (1:7) + rnorm(7, sd = 0.2),  # Happiness for professors with noise
                1.5 + 0.15 * (1:7) + rnorm(7, sd = 0.2))  # Happiness for students with noise
)


ggplot(cookies_data, aes(x = cookies, y = happiness)) +
  geom_point( size = 3) +  # Points still colored by group for clarity
  geom_smooth(method = "lm", se = FALSE, color = "#0074D9") +  # Single regression line for the entire dataset
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())


```


## ¿Qué pasa si diferenciamos entre profesores y estudiantes?

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

ggplot(cookies_data, aes(x = cookies, y = happiness)) +
  geom_point(aes(color = group), size = 3) +  # Points still colored by group for clarity
  geom_smooth(method = "lm", se = FALSE, color = "#0074D9") +  # Single regression line for the entire dataset
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad", color = "Grupo") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_color_manual(values = c("Profesores" = "navy", "Estudiantes" = "darkred")) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())


```

## Ambos grupos parecen tener pendientes diferentes...


```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

ggplot(cookies_data, aes(x = cookies, y = happiness, color = group)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE) +  # Add linear regression lines
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad", color = "Grupo") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_color_manual(values = c("Profesores" = "navy", "Estudiantes" = "darkred")) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())

```


## Variables Categóricas {.medium}

¿Cómo interpretar el modelo si agregamos una variable categórica $Estudiante$ que sea igual a 1 si la observación es de un estudiante?

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas+\hat{\beta_2} \times Estudiante$$

:::{.fragment}

- EL intercepto para las observaciones de los profesores será $\hat{\beta_0}$ porque $Estudiante=0$

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas$$

:::


## Variables Categóricas {.medium}

¿Cómo interpretar el modelo si agregamos una variable categórica $Estudiante$ que sea igual a 1 si la observación es de un estudiante?

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas+\hat{\beta_2} \times Estudiante$$


- El intercepto para las observaciones de los estudiantes será $\hat{\beta_0}+\hat{\beta_2}$ porque $Estudiante=1$

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas+\hat{\beta_2}$$


## 💪 Ejercicio 3  {.medium}

1. Estimen la siguiente regresión con el nombre `hollywood_model_2`:

$$
\widehat{\text{US Gross}} = \hat{\beta_0} + \hat{\beta_1} \text{Opening Gross} + \hat{\beta_2} \text{Budget} + \hat{\beta_3} \text{Sequel}
$$

2. Usando la función `tidy` respondan: ¿cuáles coeficientes son estadísticamente diferentes de cero?

3. Completen el siguiente código para predecir el recaudo total para una secuela con recaudo del fin de semana promedio y presupuesto promedio:

```{r}
#| echo: true
#| eval: false
multiples_valores <- data.frame(opening_gross=_________,
                             budget=__________,
                             sequel=__)
predict(hollywood_model_2, newdata = multiples_valores)

```


## Predicción en Regresión Múltiple {.medium}

```{r}
#| echo: false
hollywood_model <- lm(us_gross ~ opening_gross + budget + sequel, data=hollywood)
```

Así como en el caso con una variable, usamos el comando `predict()` para predecir $\widehat{\text{US Gross}}$. En este caso, necesitamos al menos un valor para cada variable que está en la regresión.


Hagamos una predicción para una de las observaciones en nuestros datos. En este caso, para la película "The Holiday".

```{r}
#| echo: true
#| eval: true
# Seleccionemos las 3 variables dependientes para The Holiday
the_holiday <- hollywood |> 
  filter(movie=="The Holiday") |>
  select(opening_gross, budget, sequel)

# Usamos el comando predict nuevamente
predicho <- predict(hollywood_model, newdata = the_holiday)
predicho
```


## Predicción en Regresión Múltiple {.medium}


- ¿Es precisa nuestra estimación?

```{r}
#| echo: true
#| eval: true
# El valor observado
observado <- hollywood |>
  filter(movie == "The Holiday") |>
  pull(us_gross)
observado
```

- El residuo para "The Holiday" será:

```{r}
#| echo: true
#| eval: true
observado-predicho
```




## Filtrar la variación {.medium}

- Cada **X** en el modelo explica una porción de la variación en **Y**

- La interpretación acá es más complicada que en el modelo de regresión simple porque sólo se puede mover una variable a la vez


## Interpretación para variables continuas {.medium}

Manteniendo todo lo demás constante, un incremento de una unidad en **X** está asociado con un incremento/reducción promedio de $\beta_n$ en **Y**

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel}
\end{aligned}
$$

:::{.fragment}
Manteniendo todo lo demás constante, un incremento de un dólar en el recaudo del primer fin de semana está asociado con un incremento promedio de 2.99 dólares en el recaudo total en US
:::


## Interpretación para variables categóricas {.medium}

Manteniendo todo lo demás constante, **Y** es, en promedio, $\beta_n$ unidades mayor/menor para **X**<sub>n</sub> comparado con **X**<sub>omitida</sub>

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel}
\end{aligned}
$$


:::{.fragment}

Manteniendo todo lo demás constante, las sequelas están asociadas a un recaudo promedio menor, en aproximadamente $11.9 millones, comparadas con las películas que no son secuelas

:::


## Variable categóricas con más de 2 niveles {.medium}

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel} - 15,000,000 \times \text{Trilogy}
\end{aligned}
$$

Si es la primera película $Sequel=Trilogy=0$, el modelo es:

$$\widehat{\text{US Gross}} = -8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget}$$


## Variable categóricas con más de 2 niveles {.medium}

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel} - 15,000,000 \times \text{Trilogy}
\end{aligned}
$$


Si es trilogía, entonces $Sequel=0$ y $Trilogy=1$. En este caso, el modelo es:

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 15,000,000 
\end{aligned}
$$

Manteniendo lo demás constante, estimamos que una trilogía tendrá, en promedio, un recaudo 15 millones de dólares menor que una primera entrega


## ¿Qué tan bueno es el modelo? {.medium}

### R-Squared

El $R^2$ es el porcentaje de la varianza de la variable dependiente explicada por el modelo de regresión

$$R^2=Corr(x,y)^2=Corr(y,\hat{y})$$

- Está entre 0 (nuestro modelo no predice nada) y 1 (predicción perfecta)

- No tiene unidad de medida



## ¿Qué tan bueno es el modelo? {.medium}

Con la función `glance()` podemos ver diferentes aspectos que evalúan el modelo:

```{r}
#| echo: true
glance(hollywood_model)
```

:::{.fragment}
Este modelo de regresión explica el 79% de la varianza del recaudo total en US

```{r}
#| echo: true
hollywood_model <- lm(us_gross ~ opening_gross + budget + sequel, data=hollywood)
```
:::



