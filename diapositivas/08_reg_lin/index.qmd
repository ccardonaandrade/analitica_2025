---
title: Analítica de los Negocios
author: Carlos Cardona Andrade
subtitle: Intro a la Regresión Lineal
execute:
  freeze: auto
  echo: true
  fig-width: 6
  fig-height: 5
format:
  revealjs: 
   theme: ../slides.scss
   header-includes: |
      <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" rel="stylesheet"/>
   slide-number: true
   show-slide-number: all
   transition: fade
   progress: true
   multiplex: false
   scrollable: false
   preview-links: false
   hide-inactive-cursor: true
   highlight-style: printing
   pause: true
---


```{r}
#| eval: true
#| echo: false
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel,gtools)

# Define pink color
red_pink <- "#e64173"

theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, 0, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)


# Set population and sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))


# Simulation

```



## Plan para hoy

1. [Visualizar los Datos](#viz)

2. [Regresión Lineal Simple](#lm)

3. [Regresión Lineal Múltiple](#multiple)

# Visualizar los Datos {#viz}


## ¿Por qué es importante graficar los datos? {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

library(tidyverse)
library(readxl)
library(janitor)
hollywood <- read_excel("C:/Users/ccard/Downloads/KEL702-XLS-ENG.xls", sheet = "Exhibit 1")
hollywood <- hollywood %>%
  clean_names()
hollywood <- hollywood %>% rename(us_gross = total_u_s_gross)
hollywood <- hollywood %>% rename(non_us_gross = total_non_u_s_gross)

```

```{r}
#| echo: false
#| eval: true
#| fig-align: center

ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
    geom_point() +
    labs(
        x = "Opening Gross (in millions)",
        y = "US Total Gross (in millions)"
    ) +
    theme_minimal()

```

## ¿Por qué es importante graficar los datos? {.medium}

- Podemos utilizar un diagrama de dispersión para realizar un primer análisis de la relación entre dos variables

- El coeficiente de correlación (lineal) es utilizado para medir la fuerza de la asociación (lineal) entre dos variables

- La correlación entre el recaudo en US y el recaudo el primer fin de semana:





## Cuarteto de Anscombe

```{r}
#| echo: false

library(datasauRus)
library(knitr)
library(kableExtra)
anscombe_m <- data.frame()

for(i in 1:4) {
  anscombe_m <- rbind(anscombe_m, data.frame(x=anscombe[,i], y=anscombe[,i+4]))}
  
anscombe_m <- anscombe_m |> 
  mutate(set = c(rep("I",11),rep("II",11),rep("III",11),rep("IV",11)))

means <- anscombe |> 
  select(x1,y1,x2,y2,x3,y3,x4,y4) |> 
  summarise(across(everything(), mean))

sd <- anscombe |> 
  select(x1,y1,x2,y2,x3,y3,x4,y4) |> 
  summarise(across(everything(), sd))

cor <- anscombe |>
  summarise(x1=cor(x1,y1), x2=cor(x2,y2), x3=cor(x3,y3), x4=cor(x4,y4))

anscombe |> 
  mutate(obs = as.character(1:n())) |> 
  select(obs,x1,y1,x2,y2,x3,y3,x4,y4) |>
  add_row(obs="Mean",round(means,2)) |>
  add_row(obs="SD",round(sd,2)) |>
  add_row(obs="Corr",round(cor,2)) |>
  kbl(escape = FALSE, col.names = c("Obs.", rep(c("X","Y"),4)), align="rcccccccc") |>
  column_spec(1, border_right = T, bold = T) |>
  column_spec(seq(3,7,2), border_right = T) |>
  row_spec(11, extra_css = "border-bottom: 3px solid") |>
  add_header_above(c(" " = 1, "I" = 2, "II"=2, "III"=2, "IV"=2)) |>
  kable_classic(full_width = F)
```


## ¿Qué aprendemos cuando graficamos los datos?

```{r}
#| echo: false
#| fig-align: center
ggplot(anscombe_m, aes(x, y)) +  
geom_text(aes(x = 15, y = 5, label=set), family = "Alfa Slab One", size=36, color = "gray95") + 
geom_smooth(method="lm", fill=NA, fullrange=TRUE, color = "black", linewidth=0.5) + 
geom_point(size=3.5, color="black", fill="red", alpha=0.8, shape=21) +
facet_wrap(~set, ncol=2, scales = "free") +
scale_x_continuous(limits = c(0,20)) +
scale_y_continuous(limits = c(0,15)) +
labs(x = NULL, y = NULL) +
coord_cartesian(expand = F) +
theme_minimal() +
theme(strip.text = element_blank(),
axis.text = element_blank(),
panel.border = element_rect(linewidth = 0.1, fill = NA))
```

## ¿Ven correlación acá?

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 8
datasaurus_dozen |>
  filter(dataset %in% c("slant_down", "dino")) |>
  mutate(dataset = ifelse(dataset == "dino", "B", "A")) |>
  ggplot(aes(x = x, y = y, colour = dataset))+
  geom_point() +
  scale_color_manual(values = c("#C32402", "#0234C3")) +
  facet_wrap(~dataset, ncol = 2) +
  labs(x=NULL, y=NULL) +
  theme_minimal() +
  theme(legend.position = "none",
  strip.text = element_blank(),
  panel.border = element_rect(linewidth = 0.1, fill = NA))
```

. . .

::: {.columns}
::: {.column style="text-align:center;"}
[Correlación: -0.07]{.secfont style="font-size:1.5rem;color:#C32402;"}
:::
::: {.column style="text-align:center;"}
[Correlación: -0.07]{.secfont style="font-size:1.5rem;color:#0234C3;"}
:::
:::


## Iguales pero diferentes

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 10
datasaurus_dozen |>
  filter(!dataset == "dino") |>
  ggplot(aes(x = x, y = y, colour = dataset))+
  geom_point() +
  geom_text(aes(x = 50, y = -10, label = glue::glue("Cor.: {round(cor(x,y),2)}")), hjust = 0.5) +
  facet_wrap(~dataset, nrow = 2) +
  labs(x=NULL, y=NULL) +
  theme_minimal() +
  theme(legend.position = "none",
  strip.text = element_blank(),
  axis.text = element_blank(),
  panel.border = element_rect(linewidth = 0.1, fill = NA))
```


## ¿Por qué es importante graficar los datos? {.medium}

- [Visualización de patrones]{.hl .hl-blue}: Los gráficos permiten identificar tendencias, patrones y relaciones entre variables que pueden no ser evidentes solo con los números

. . .

- [Análisis visual vs. numérico]{.hl .hl-blue}: Aunque las estadísticas (media, desviación estándar, correlación) pueden ser iguales entre los conjuntos de datos, los gráficos muestran que los datos pueden comportarse de manera muy diferente

  - Ejemplo: Cuarteto de Ascombe

. . .
  
- En conclusión, la visualización de datos no solo facilita la comprensión de la información, sino que también ayuda a evitar interpretaciones erróneas basadas únicamente en estadísticas numéricas 


## 💪 Ejercicio 1  {.medium}

1. Carguen el paquete `tidyverse`, `janitor` y `corrplot` a R. 

2. Ejecuten el siguiente código para importar los datos del caso *Hollywood Rules*:

```{r}
#| echo: true
#| eval: false
hollywood <- read_excel("KEL702-XLS-ENG.xls", sheet = "Exhibit 1")
```


3. Limpiemos los nombres con el paquete `janitor` para que sean legibles:

```{r}
#| echo: true
#| eval: false
hollywood <- hollywood |>
  clean_names()
hollywood <- hollywood |>
  rename(us_gross = total_u_s_gross,
         non_us_gross = total_non_u_s_gross)
```


## 💪 Ejercicio 1  {.medium}

4. Creen un nuevo conjunto de datos que se llame `hollywood_sub` que *sólo* contenga las variables de recaudo en USA, el recaudo en el estreno, el recaudo en el mercado internacional y el presupuesto en **ese orden**

5. Grafiquemos la matriz de correlación con el siguiente código:

```{r}
#| echo: true
#| eval: false
# Matriz de correlaciones
cor_matrix <- cor(hollywood_sub)

# Renombrar columnas y filas en la matriz
colnames(cor_matrix) <- c("US Gross", "Opening Gross", "Non-US Gross", "Budget")
rownames(cor_matrix) <- c("US Gross", "Opening Gross", "Non-US Gross", "Budget")

# Crear el gráfico de correlaciones
corrplot(cor_matrix, method = "number", type = "upper")
```

## 💪 Ejercicio 1  {.medium}

6. Analicen las dos variables que tengan la correlación más alta y las que tengan la correlación más baja.

7. Creen el gráfico de dispersión entre el recaudo total y el recaudo en el día del estreno como en la diapositiva 1. ¿Es consistente el gráfico con el valor de la correlación?

    
    
# Regresión Lineal Simple {#lm}

## ¿Por qué una regresión? {.medium}

### Ejemplo

El promedio en la universidad es resultado de la habilidad y las horas estudiadas. Por lo tanto, uno podría pensar en el siguiente modelo:

$$Promedio=f(H,Saber11,PCA)$$
Donde $H$ es la habilidad, $Saber11$ es el puntaje en la prueba Saber 11 y $PCA$ es el porcentaje de clases a las que el estudiante fue. Esperamos que el $Promedio$ aumente cada que alguna de esas 3 variables aumente.

:::{.fragment}

Sin embargo, no necesitamos esperar. Podemos evaluar esta hipótesis usando un **modelo de regresión!**

:::

## ¿Por qué una regresión? {.medium}

**Modelo de Regresión:**

$$Promedio_i=\beta_0 + \beta_1H_i + \beta_2Saber11_i+\beta_3PCT_i+\varepsilon_i$$
Queremos estimar/evaluar la relación 
$$Promedio=f(H,Saber11,PCA)$$

. . .

**Preguntas a responder:**

- ¿Cómo se interpretan $\beta_0$, $\beta_1$, $\beta_2$ y $\beta_3$?

- ¿Son los términos $\beta_k$ parámetros o son estimaciones muestrales?

- ¿Qué es $\varepsilon_i$?
     
## Partes esenciales de una regresión {.medium}

$$y_i=\beta_0+\beta_1x_i+e_i$$

::: columns
::: {.column width="50%"}
### $y$  {.text-orange-gold .center}

- Variable dependiente

- Variable resultado

- Básicamente lo que queremos predecir o explicar

:::

::: {.column .fragment width="50%"}

### $x$  {.text-orange-gold .center}

- Variable explicativa

- Predictor

- Variable independiente

- Lo que usamos para predecir o explicar $y$
:::
:::


## Objetivos de una regresión {.medium}

Usualmente ajustamos a una línea por dos razones:

### Predicción

- Predecir el futuro

- Nos enfocamos en $y$

- Netflix tratando de predecir la siguiente serie que veremos

### Explicación

- Explicar el efecto de $x$ en $y$

- Nos enfocamos en $x$

- Netflix evaluando el efecto de la hora del día en la selección de una serie


## ¿Cómo se estima la línea de regresión? {.medium}

1. Graficar ambas variables $y$ y $x$

2. Dibujar una recta que se aproxime a la relación observada (ojalá funcione para datos que no están en la muestra)

3. Estimar los números que componen esa recta

4. Interpretar esos números

```{r}
#| echo: false
library(tidyverse)
library(patchwork)
library(broom)
library(knitr)
galletas <- tibble(felicidad = c(0.5, 2, 1, 2.5, 3, 1.5, 2, 2.5, 2, 3),
                  galletas = 1:10)

galletas_datos <- galletas
modelo_galletas <- lm(felicidad ~ galletas, data = galletas_datos)
predict_galletas <- augment(modelo_galletas)
```



## Galletas y Felicidad {.medium}

::: {.tbl-classic .tbl-larger .center-text}


```{r}
#| echo: false
#| eval: true

galletas |> 
  knitr::kable(format = "html") |> 
  kableExtra::kable_styling(font_size = 30) # Adjust the font size as needed
```

:::

## ¿Cómo se relacionan las galletas con la felicidad? {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4



base_cookies <- ggplot(predict_galletas, aes(x = galletas, y = felicidad)) +
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

base_cookies
```




## Trazando una curva que sigue los datos {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = lm, color = "#0074D9", formula = y ~ splines::bs(x, 7), se = FALSE)
```

## Otra línea que se adapta a los datos {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = "loess", color = "#0074D9", se = FALSE)
```

## Ajuste con una línea recta (Regresión lineal) {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE)
```

## Errores de predicción {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

galletas_con_residuo <- base_cookies +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE) +
  geom_segment(aes(xend = galletas, yend = .fitted), color = "#FF851B", size = 1)

galletas_con_residuo
```

## Visualización de los errores

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

galletas_solo_residuo <- ggplot(predict_galletas, aes(x = galletas, y = .resid)) +
  geom_hline(yintercept = 0, color = "#B10DC9", size = 1) +
  geom_point(size = 3) +
  geom_segment(aes(xend = galletas, yend = 0), color = "#FF851B", size = 1) +
  coord_cartesian(xlim = c(0, 10), ylim = c(-1.5, 1.5)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Galletas consumidas", y = "Distancia a la línea") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

galletas_solo_residuo
```

##


```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

(galletas_con_residuo + labs(title = "Galletas y Felicidad")) + 
  (galletas_solo_residuo + labs(title = "Errores"))
```

## Pendiente de una recta {.medium}

$$
y = mx + b
$$


- $y$ es un número

- $x$ es un número

- $m$ es la [pendiente]{.marker-hl}  $\frac{\Delta y}{\Delta x}$

- $b$ es el [intercepto]{.marker-hl} con $y$

## Pendiente de una recta: Ejemplos {.medium}

::: columns
::: {.column width="50%"}
$$
y = 2x - 1
$$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 4.8
#| fig-height: 3.5

ggplot(data = tibble(x = 0:5), aes(x = x)) +
  stat_function(fun = function(x) 2 * x - 1, 
                color = "#BF3984", size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:5) +
  scale_y_continuous(breaks = -1:9) +
  theme(panel.grid.minor = element_blank())
```

:::



::: {.column .fragment width="50%"}
$$
y = -0.5x + 6
$$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 4.8
#| fig-height: 3.5

ggplot(data = tibble(x = 0:14), aes(x = x)) +
  stat_function(fun = function(x) -0.5 * x + 6, 
                color = "#BF3984", size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:14) +
  scale_y_continuous(breaks = -1:9) +
  theme(panel.grid.minor = element_blank())
```

:::
:::

## Partes esenciales de una regresión II {.medium}

$$
y = \beta_0 + \beta_1 x + \varepsilon
$$

-   $\beta_1$: la pendiente [verdadera]{.marker-hl}  de la relación entre $x$ y $y$

-   $\beta_0$: el intercepto [verdadero]{.marker-hl}  de la relación entre $x$ y $y$

-   $\varepsilon$: el error

## Partes esenciales de una regresión II {.medium}

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x 
$$

- $\hat{y}$: es la estimación de la variable dependiente dado nuestro modelo

- $\hat{\beta_1}$: la pendiente [estimada]{.marker-hl}  de la relación entre $x$ y $y$

- $\hat{\beta_0}$: el intercepto [estimada]{.marker-hl}  de la relación entre $x$ y $y$

- No hay error!!!

. . .

- [El símbolo *gorro* ($\hat{\beta_k}$ o $\hat{y}$) significa que es una estimación y no un parámetro real]{.hl .hl-dred}

## Modelo de Regresión {.medium}

::: columns
::: {.column width="40%"}
$$  
  \begin{aligned}
y &= \color{#0074D9}{\text{Modelo}} + \color{#FF851B}{\text{Error}} \\
  &= \color{#0074D9}{f(x)} + \color{#FF851B}{\varepsilon}
\end{aligned}
$$
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| out-width: "100%"


galletas_con_residuo
```
:::
:::

## Los errores {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| out-width: "100%"


galletas_con_residuo

```

$$
\color{#FF851B}{\text{Error}} = \text{Observado} - \color{#0074D9}{\text{Predicho}} = y - \color{#0074D9}{\hat{y}}
$$

## La línea de los mínimos cuadrados ordinarios (MCO) {.medium}

-   El error para la observación $i$ es:

$$e_i= \text{Observado} - \text{Predicho}=y_i - \hat{y_i}$$ - La **suma de los residuos al cuadrado** es:

$$e_1^2+e_2^2+e_3^2+..+e_n^2$$

-   La **línea de los mínimos cuadrados ordinarios** es la que minimiza la suma de los residuos al cuadrado:

$$ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \sum e_i^2 $$


## 

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/meme.png")

```


## MCO vs Otras líneas {.smaller}

Volvamos a la relación felicidad y galletas...

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

Para cualquier línea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

Para cualquier línea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

Para cualquier línea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

Para cualquier línea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

$\sum e_i^2$ calcula el cuadrado de los errores: errores más grandes reciben una penalización mayor

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```


## MCO vs Otras líneas {.smaller}

La línea de MCO es la combinación de $\hat{\beta_0}$ y $\hat{\beta_1}$ que minimizan $\sum e_i^2$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

## ¿Cómo gráficar la línea de regresión? {.medium}

`geom_smooth(method="lm")`es la función dentro de ggplot para gráficar la línea de regresión y su respectivo intervalo de confianza.

::: columns
::: {.column width="40%"}

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "5" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::
:::


## ¿Cómo gráficar la línea de regresión? {.medium}

La opción `se = FALSE` elimina los intervalos de confianza.

::: columns
::: {.column width="40%"}

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "5" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE) +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "4" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE) +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::
:::




## Estimar una regresión en R {.medium}

-   La función `lm()` estima una regresión lineal y la función `summary()` reporta los resultados:

```{r}
#| eval: false
#| echo: true

name_of_model <- lm(y ~ x, data = DATA)

summary(name_of_model)  # Para ver los detalles del modelo
```

::: fragment

-   La función `tidy` del paquete `broom` reporta  los resultados del modelo como un data frame para graficar:

```{r}
#| eval: false
#| echo: true

library(broom)

tidy(name_of_model)
```
:::

## Modelando Galletas y Felicidad {.medium}

::: columns
::: {.column width="40%"}
$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas$$

```{r}
#| echo: true
#| eval: true
modelo_felicidad <- 
  lm(felicidad ~ galletas,
     data = galletas_datos)
```
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| label: cookies-happiness-again
#| results: hide


base_cookies +
  geom_smooth(method = "lm", color = "#0074D9") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::
:::

## Modelando Galletas y Felicidad {.medium}

La función `tidy` nos permite ver los:

  - coeficientes
  - errores estándar
  - el estadístico $t$
  - el p-value
  - los IC
  
. . .
  
Para nuestro `modelo_felicidad`:

```{r}
#| echo: true
tidy(modelo_felicidad, conf.int = TRUE)
```



## Traduciendo los resultados a matemáticas {.medium}

::: columns
::: {.column width="40%"}
```{r}
#| echo: false
tidy(modelo_felicidad, conf.int = TRUE) |> 
  select(term, estimate)
```

$$
\begin{aligned}
&\widehat{Felicidad} = \\ 
&\hat{\beta_0}+\hat{\beta_1}\times Galletas
\end{aligned}
$$

$$
\begin{aligned}
&\widehat{Felicidad} = \\ 
&1.1 + 0.16 \times Galletas
\end{aligned}
$$
:::

::: {.column width="60%"}
```{r}
#| echo: false
base_cookies +
  geom_smooth(method = "lm", color = "#0074D9") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::
:::

## Interpretación de los coeficientes {.medium}

Un incremento en una unidad de $X$ está *asociado* con un incremento (o reducción) promedio de $\beta_1$ unidades en $Y$

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas$$

$$\widehat{Felicidad} = 1.1 + 0.16 \times Galletas$$

:::{.incremental}

- En *promedio*, una galleta adicional está asociado a aumento en la felicidad de 0.16 unidades

- Si no hay consumo de galletas, esperamos que el puntaje de felicidad sea 1.1 unidades
:::


## ¿Es el intercepto importante? {.medium}

- La interpretación del intercepto es importante si en el contexto de los datos:

  1. La variable independiente puede tomar valores iguales o cercanos a cero
  
  2. La variable independiente tiene valores cercanos a cero en los datos observados
  
:::{.incremental}
  
- En caso contrario, el intercepto no tiene ninguna interpretación práctica

- Veremos más ejemplos sobre esto más adelante...
:::


## 💪 Ejercicio 2  {.medium}

Según la sabiduría popular en Hollywood, el recaudo durante el primer fin de semana es un fuerte predictor del éxito comercial de una película.

1. Grafiquen la línea de regresión entre el recaudo total en Estados Unidos y el recaudo en el primer fin de semana para evaluar esta creencia.

2. Estimen la siguiente regresión y llámenla `hollywood_model`:

$$\widehat{\text{US Total Gross}} = \hat{\beta_0} + \hat{\beta_1} \times \text{Opening Gross}$$
3. Usando la función `tidy` reporten los valores de los coeficientes.

4. ¿Cuál es la interpretación de $\hat{\beta_1}$ en este caso?¿Y de $\hat{\beta_0}$?
 



## Predicción  {.medium}

```{r}
#| echo: false
#| eval: true
hollywood_model <- lm(us_gross ~ opening_gross, data=hollywood)
```

$$\widehat{\text{US Total Gross}} = 5,108,220 + 3.12 \times \text{Opening Gross}$$


Según nuestro modelo, ¿cuál sería el recaudo en US de una película cuyo recaudo en el primer fin de semana fue de \$50,000,000?



$$  
  \begin{aligned}
\widehat{\text{US Gross}} &= 5,108,220 + 3.12 \times \text{Opening Gross} \\
  &= 5,108,220 + 3.12 \times \color{red}{50,000,000} \\
  &= 161,108,220
\end{aligned}
$$

## Predicción con R {.medium}

El comando `predict()` nos permite predecir $\widehat{\text{US Gross}}$ para uno o varios valores:



```{r}
#| echo: true
#| eval: true

# Creamos los valores para los cuales queremos predecir
valores_opening <- data.frame(opening_gross = c(20000000,40000000,50000000))


# Predice los valores con los coeficientes estimados
# en hollywood_model
predict(hollywood_model, newdata = valores_opening)

```


## Predicción  {.medium}


```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-height: 4
#| fig-width: 8

model <- lm(us_gross ~ opening_gross , data = hollywood)

# Predict the corresponding y value when x = 50
predicted_y <- predict(model, newdata = data.frame(opening_gross = 50 * 1000000))

# Plot
ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_vline(xintercept = 50, linetype = "dashed", color = "blue") +   # Dashed vertical line at x = 50
  geom_hline(yintercept = predicted_y/ 1000000, linetype = "dashed", color = "blue") + # Dashed horizontal line at predicted y
  geom_point(aes(x = 50, y = predicted_y/ 1000000), color = "red", size = 3) + # Point at (50, predicted_y)
  labs(
    x = "Opening Gross (in millions)",
    y = "US Total Gross (in millions)"
  ) +
  theme_minimal()
```





## ¿Es posible la extrapolación? {.medium}

Extrapolar es tratar de predecir Y fuera del rango de valores de X. Es posible pero no aconsejable.

```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-height: 4
#| fig-width: 8
# Predict the corresponding y value when x = 80
predicted_y <- predict(model, newdata = data.frame(opening_gross = 80 * 1000000))

# Plot
ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
    geom_point() +
    geom_smooth(method = "lm") +
    geom_vline(xintercept = 80, linetype = "dashed", color = "blue") +   # Dashed vertical line at x = 80
    geom_hline(yintercept = predicted_y/ 1000000, linetype = "dashed", color = "blue") + # Dashed horizontal line at predicted y
    geom_point(aes(x = 80, y = predicted_y/ 1000000), color = "red", size = 3) + # Point at (80, predicted_y)
    labs(
        x = "Opening Gross (in millions)",
        y = "US Total Gross (in millions)"
    ) + theme_minimal()

```


## Inferencia de los coeficientes {.medium}

Cuando trabajamos con distribuciones muestrales, la idea era que:

$$
\bar{X} \xrightarrow{\text{🤞 ojalá 🤞}} \mu
$$

:::{.fragment}

De igual manera, en el modelo de regresión queremos:

$$
\hat{\beta} \xrightarrow{\text{🤞 ojalá 🤞}} \beta
$$
:::



## Inferencia de los coeficientes {.medium}

$$\widehat{\text{US Total Gross}} = 5,108,220 + 3.12 \times \text{Opening Gross}$$

- Es $\beta_1$ diferente de cero?


::: fragment
```{r}
#| echo: true
tidy(hollywood_model, conf.int = TRUE)
```
:::


## Más pruebas de hipótesis {.medium}

$$H_0:\beta_1=0$$
$$H_1: \beta_1 \neq 0$$

:::{.fragment}

$$t=\dfrac{3.12-0}{0.218}=14.3>t_{\alpha=0.05}=1.96$$

- Rechazamos la $H_0$ a un nivel de significancia del 5%! 

- El p-value es menor a 0.05:

```{r}
#| echo: true
tidy(hollywood_model, conf.int = TRUE)[2,5]<0.05
```

:::


# Regresión Lineal Múltiple {#multiple}

## Regresión Múltiple {.medium}

No estamos limitados a una sola variable explicativa!

$$
\hat{y} = \hat{\beta_0}  + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_n} x_n 
$$

:::{.fragment}
Podríamos pensar que el verdadero modelo de felicidad es:

  $$\widehat{\text{Felicidad}}=\hat{\beta_0}+\hat{\beta_1}\times \text{Galletas}+\hat{\beta_2} \times \text{Estudiante}$$

O que para predecir el recaudo de una película se necesita el siguiente modelo:

$$
\widehat{\text{US Gross}} = \hat{\beta_0} + \hat{\beta_1} \text{Opening Gross} + \hat{\beta_2} \text{Budget} + \hat{\beta_3} \text{Sequel}
$$

:::



## ¿Cómo pensar visualmente la relación $y=f(x_1,x_2)+\varepsilon$? {.medium}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/dots.png")

```


## ¿Cómo pensar visualmente la "recta" en una regresión múltiple? {.medium}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/plane.png")

```



## Variables Categóricas vs Variables Continuas {.medium}

En los dos modelos antes explicados hay dos tipos de variables:

### Variables Categóricas

- $\text{Estudiante}$

- $\text{Sequel}$

### Variables Continuas

- $\text{Galletas}$

- $\text{Opening Gross}$



## Variables Categóricas vs Variables Continuas {.medium}

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/slider-switch-plain-80.jpg")

```


## Variables Categóricas vs Variables Continuas {.medium}

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/slider-switch-annotated-80.jpg")

```


## Ejemplo de felicidad y galletas

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

set.seed(123)

# Create a tibble with the same slope but different intercepts for professors and students, adding noise
cookies_data <- tibble(
  cookies = rep(1:7, 2),  # Galletas consumidas (1 through 7 for both groups)
  group = rep(c("Profesores", "Estudiantes"), each = 7),  # Group variable
  happiness = c(0.5 + 0.15 * (1:7) + rnorm(7, sd = 0.2),  # Happiness for professors with noise
                1.5 + 0.15 * (1:7) + rnorm(7, sd = 0.2))  # Happiness for students with noise
)


ggplot(cookies_data, aes(x = cookies, y = happiness)) +
  geom_point( size = 3) +  # Points still colored by group for clarity
  geom_smooth(method = "lm", se = FALSE, color = "#0074D9") +  # Single regression line for the entire dataset
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())


```


## ¿Qué pasa si diferenciamos entre profesores y estudiantes?

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

ggplot(cookies_data, aes(x = cookies, y = happiness)) +
  geom_point(aes(color = group), size = 3) +  # Points still colored by group for clarity
  geom_smooth(method = "lm", se = FALSE, color = "#0074D9") +  # Single regression line for the entire dataset
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad", color = "Grupo") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_color_manual(values = c("Profesores" = "navy", "Estudiantes" = "darkred")) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())


```

## Ambos grupos parecen tener pendientes diferentes...


```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

ggplot(cookies_data, aes(x = cookies, y = happiness, color = group)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE) +  # Add linear regression lines
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad", color = "Grupo") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_color_manual(values = c("Profesores" = "navy", "Estudiantes" = "darkred")) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())

```


## Variables Categóricas {.medium}

¿Cómo interpretar el modelo si agregamos una variable categórica $Estudiante$ que sea igual a 1 si la observación es de un estudiante?

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas+\hat{\beta_2} \times Estudiante$$

:::{.fragment}

- El intercepto para las observaciones de los profesores será $\hat{\beta_0}$ porque $Estudiante=0$

$$\widehat{Felicidad}=\color{#FF851B}{\hat{\beta_0}}+\hat{\beta_1}\times Galletas$$

:::


## Variables Categóricas {.medium}

¿Cómo interpretar el modelo si agregamos una variable categórica $Estudiante$ que sea igual a 1 si la observación es de un estudiante?

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas+\hat{\beta_2} \times Estudiante$$


- El intercepto para las observaciones de los estudiantes será $\hat{\beta_0}+\hat{\beta_2}$ porque $Estudiante=1$

$$\widehat{Felicidad}=\color{#FF851B}{\hat{\beta_0}}+\hat{\beta_1}\times Galletas+\color{#FF851B}{\hat{\beta_2}}$$


## Estimar una regresión múltiple en R {.medium}

-   Nuevamente usamos la función `lm()`, simplemente separamos las variables independientes con un `+`:

```{r}
#| eval: false
#| echo: true

name_of_model <- lm(y ~ x1 + x2 + x3, data = DATA)

tidy(name_of_model)  
```


## 💪 Ejercicio 3  {.medium}

1. Estimen la siguiente regresión con el nombre `hollywood_model_2`:

$$
\widehat{\text{US Gross}} = \hat{\beta_0} + \hat{\beta_1} \text{Opening Gross} + \hat{\beta_2} \text{Budget} + \hat{\beta_3} \text{Sequel}
$$

2. Usando la función `tidy` respondan: ¿cuáles coeficientes son estadísticamente diferentes de cero?

3. Completen el siguiente código para predecir el recaudo total para una secuela con recaudo del fin de semana promedio y presupuesto promedio:

```{r}
#| echo: true
#| eval: false
multiples_valores <- data.frame(opening_gross=_________,
                             budget=__________,
                             sequel=__)
predict(hollywood_model_2, newdata = multiples_valores)

```


## Predicción en Regresión Múltiple {.medium}

```{r}
#| echo: false
hollywood_model <- lm(us_gross ~ opening_gross + budget + sequel, data=hollywood)
```

Así como en el caso con una variable, usamos el comando `predict()` para predecir $\widehat{\text{US Gross}}$. En este caso, necesitamos al menos un valor para cada variable que está en la regresión.


Hagamos una predicción para una de las observaciones en nuestros datos. En este caso, para la película "The Holiday":

```{r}
#| echo: true
#| eval: true
# Seleccionemos las 3 variables dependientes para The Holiday
the_holiday <- hollywood |> 
  filter(movie=="The Holiday") |>
  select(opening_gross, budget, sequel)

# Usamos el comando predict nuevamente
y_gorro <- predict(hollywood_model, newdata = the_holiday)
y_gorro
```


## Predicción en Regresión Múltiple {.medium}


- ¿Es precisa nuestra estimación? Comparemos nuestro estimado con el recaudo real de la película:

```{r}
#| echo: true
#| eval: true
# El valor observado
y <- hollywood |>
  filter(movie == "The Holiday") |>
  pull(us_gross)
y
```

. . .

- El error para "The Holiday" será:

$$e=y - \hat{y}$$

```{r}
#| echo: true
#| eval: true
y - y_gorro
```




## Filtrar la variación {.medium}

- Cada **$x$** en el modelo explica una porción de la variación en **$y$**

- La interpretación acá es más complicada que en el modelo de regresión simple porque sólo se puede mover una variable a la vez


## Interpretación para variables continuas {.medium}

Manteniendo todo lo demás constante, un incremento de una unidad en **$x$** está asociado con un incremento/reducción promedio de $\beta_k$ en **$y$**

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel}
\end{aligned}
$$

:::{.fragment}
Manteniendo todo lo demás constante, un incremento de un dólar en el recaudo del primer fin de semana está asociado con un incremento promedio de 2.99 dólares en el recaudo total en US
:::


## Interpretación para variables categóricas {.medium}

Manteniendo todo lo demás constante, **$y$** es, en promedio, $\beta_k$ unidades mayor/menor para **$x$**<sub>k</sub> comparado con **$x$**<sub>omitida</sub>

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel}
\end{aligned}
$$


:::{.fragment}

Manteniendo todo lo demás constante, las sequelas están asociadas a un recaudo promedio menor, en aproximadamente $11.9 millones, comparadas con las películas que no son secuelas

:::


## Variable categóricas con más de 2 niveles {.medium}

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel} - 15,000,000 \times \text{Trilogy}
\end{aligned}
$$

Si es la primera película $Sequel=Trilogy=0$, el modelo es:

$$\widehat{\text{US Gross}} = -8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget}$$


## Variable categóricas con más de 2 niveles {.medium}

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel} - 15,000,000 \times \text{Trilogy}
\end{aligned}
$$


Si es trilogía, entonces $Sequel=0$ y $Trilogy=1$. En este caso, el modelo es:

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 15,000,000 
\end{aligned}
$$

Manteniendo lo demás constante, estimamos que una trilogía tendrá, en promedio, un recaudo 15 millones de dólares menor que una primera entrega


