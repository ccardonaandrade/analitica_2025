---
title: Anal칤tica de los Negocios
author: Carlos Cardona Andrade
subtitle: Intro a la Regresi칩n Lineal
execute:
  freeze: auto
  echo: true
  fig-width: 6
  fig-height: 5
format:
  revealjs: 
   theme: ../slides.scss
   header-includes: |
      <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" rel="stylesheet"/>
   slide-number: true
   show-slide-number: all
   transition: fade
   progress: true
   multiplex: false
   scrollable: false
   preview-links: false
   hide-inactive-cursor: true
   highlight-style: printing
   pause: true
---


```{r}
#| eval: true
#| echo: false
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel,gtools)

# Define pink color
red_pink <- "#e64173"

theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, 0, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)


# Set population and sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))


# Simulation

```



## Plan para hoy

1. [Visualizar los Datos](#viz)

2. [Regresi칩n Lineal Simple](#lm)

3. [Regresi칩n Lineal M칰ltiple](#multiple)

# Visualizar los Datos {#viz}


## 쯇or qu칠 es importante graficar los datos? {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

library(tidyverse)
library(readxl)
library(janitor)
hollywood <- read_excel("C:/Users/ccard/Downloads/KEL702-XLS-ENG.xls", sheet = "Exhibit 1")
hollywood <- hollywood %>%
  clean_names()
hollywood <- hollywood %>% rename(us_gross = total_u_s_gross)
hollywood <- hollywood %>% rename(non_us_gross = total_non_u_s_gross)

```

```{r}
#| echo: false
#| eval: true
#| fig-align: center

ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
    geom_point() +
    labs(
        x = "Opening Gross (in millions)",
        y = "US Total Gross (in millions)"
    ) +
    theme_minimal()

```

## 쯇or qu칠 es importante graficar los datos? {.medium}

- Podemos utilizar un diagrama de dispersi칩n para realizar un primer an치lisis de la relaci칩n entre dos variables

- El coeficiente de correlaci칩n (lineal) es utilizado para medir la fuerza de la asociaci칩n (lineal) entre dos variables

- La correlaci칩n entre el recaudo en US y el recaudo el primer fin de semana:





## Cuarteto de Anscombe

```{r}
#| echo: false

library(datasauRus)
library(knitr)
library(kableExtra)
anscombe_m <- data.frame()

for(i in 1:4) {
  anscombe_m <- rbind(anscombe_m, data.frame(x=anscombe[,i], y=anscombe[,i+4]))}
  
anscombe_m <- anscombe_m |> 
  mutate(set = c(rep("I",11),rep("II",11),rep("III",11),rep("IV",11)))

means <- anscombe |> 
  select(x1,y1,x2,y2,x3,y3,x4,y4) |> 
  summarise(across(everything(), mean))

sd <- anscombe |> 
  select(x1,y1,x2,y2,x3,y3,x4,y4) |> 
  summarise(across(everything(), sd))

cor <- anscombe |>
  summarise(x1=cor(x1,y1), x2=cor(x2,y2), x3=cor(x3,y3), x4=cor(x4,y4))

anscombe |> 
  mutate(obs = as.character(1:n())) |> 
  select(obs,x1,y1,x2,y2,x3,y3,x4,y4) |>
  add_row(obs="Mean",round(means,2)) |>
  add_row(obs="SD",round(sd,2)) |>
  add_row(obs="Corr",round(cor,2)) |>
  kbl(escape = FALSE, col.names = c("Obs.", rep(c("X","Y"),4)), align="rcccccccc") |>
  column_spec(1, border_right = T, bold = T) |>
  column_spec(seq(3,7,2), border_right = T) |>
  row_spec(11, extra_css = "border-bottom: 3px solid") |>
  add_header_above(c(" " = 1, "I" = 2, "II"=2, "III"=2, "IV"=2)) |>
  kable_classic(full_width = F)
```


## 쯈u칠 aprendemos cuando graficamos los datos?

```{r}
#| echo: false
#| fig-align: center
ggplot(anscombe_m, aes(x, y)) +  
geom_text(aes(x = 15, y = 5, label=set), family = "Alfa Slab One", size=36, color = "gray95") + 
geom_smooth(method="lm", fill=NA, fullrange=TRUE, color = "black", linewidth=0.5) + 
geom_point(size=3.5, color="black", fill="red", alpha=0.8, shape=21) +
facet_wrap(~set, ncol=2, scales = "free") +
scale_x_continuous(limits = c(0,20)) +
scale_y_continuous(limits = c(0,15)) +
labs(x = NULL, y = NULL) +
coord_cartesian(expand = F) +
theme_minimal() +
theme(strip.text = element_blank(),
axis.text = element_blank(),
panel.border = element_rect(linewidth = 0.1, fill = NA))
```

## 쯌en correlaci칩n ac치?

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 8
datasaurus_dozen |>
  filter(dataset %in% c("slant_down", "dino")) |>
  mutate(dataset = ifelse(dataset == "dino", "B", "A")) |>
  ggplot(aes(x = x, y = y, colour = dataset))+
  geom_point() +
  scale_color_manual(values = c("#C32402", "#0234C3")) +
  facet_wrap(~dataset, ncol = 2) +
  labs(x=NULL, y=NULL) +
  theme_minimal() +
  theme(legend.position = "none",
  strip.text = element_blank(),
  panel.border = element_rect(linewidth = 0.1, fill = NA))
```

. . .

::: {.columns}
::: {.column style="text-align:center;"}
[Correlaci칩n: -0.07]{.secfont style="font-size:1.5rem;color:#C32402;"}
:::
::: {.column style="text-align:center;"}
[Correlaci칩n: -0.07]{.secfont style="font-size:1.5rem;color:#0234C3;"}
:::
:::


## Iguales pero diferentes

```{r}
#| echo: false
#| fig-align: center
#| fig-width: 10
datasaurus_dozen |>
  filter(!dataset == "dino") |>
  ggplot(aes(x = x, y = y, colour = dataset))+
  geom_point() +
  geom_text(aes(x = 50, y = -10, label = glue::glue("Cor.: {round(cor(x,y),2)}")), hjust = 0.5) +
  facet_wrap(~dataset, nrow = 2) +
  labs(x=NULL, y=NULL) +
  theme_minimal() +
  theme(legend.position = "none",
  strip.text = element_blank(),
  axis.text = element_blank(),
  panel.border = element_rect(linewidth = 0.1, fill = NA))
```


## 쯇or qu칠 es importante graficar los datos? {.medium}

- [Visualizaci칩n de patrones]{.hl .hl-blue}: Los gr치ficos permiten identificar tendencias, patrones y relaciones entre variables que pueden no ser evidentes solo con los n칰meros

. . .

- [An치lisis visual vs. num칠rico]{.hl .hl-blue}: Aunque las estad칤sticas (media, desviaci칩n est치ndar, correlaci칩n) pueden ser iguales entre los conjuntos de datos, los gr치ficos muestran que los datos pueden comportarse de manera muy diferente

  - Ejemplo: Cuarteto de Ascombe

. . .
  
- En conclusi칩n, la visualizaci칩n de datos no solo facilita la comprensi칩n de la informaci칩n, sino que tambi칠n ayuda a evitar interpretaciones err칩neas basadas 칰nicamente en estad칤sticas num칠ricas 


## 游눩 Ejercicio 1  {.medium}

1. Carguen el paquete `tidyverse`, `janitor` y `corrplot` a R. 

2. Ejecuten el siguiente c칩digo para importar los datos del caso *Hollywood Rules*:

```{r}
#| echo: true
#| eval: false
hollywood <- read_excel("KEL702-XLS-ENG.xls", sheet = "Exhibit 1")
```


3. Limpiemos los nombres con el paquete `janitor` para que sean legibles:

```{r}
#| echo: true
#| eval: false
hollywood <- hollywood |>
  clean_names()
hollywood <- hollywood |>
  rename(us_gross = total_u_s_gross,
         non_us_gross = total_non_u_s_gross)
```


## 游눩 Ejercicio 1  {.medium}

4. Creen un nuevo conjunto de datos que se llame `hollywood_sub` que *s칩lo* contenga las variables de recaudo en USA, el recaudo en el estreno, el recaudo en el mercado internacional y el presupuesto en **ese orden**

5. Grafiquemos la matriz de correlaci칩n con el siguiente c칩digo:

```{r}
#| echo: true
#| eval: false
# Matriz de correlaciones
cor_matrix <- cor(hollywood_sub)

# Renombrar columnas y filas en la matriz
colnames(cor_matrix) <- c("US Gross", "Opening Gross", "Non-US Gross", "Budget")
rownames(cor_matrix) <- c("US Gross", "Opening Gross", "Non-US Gross", "Budget")

# Crear el gr치fico de correlaciones
corrplot(cor_matrix, method = "number", type = "upper")
```

## 游눩 Ejercicio 1  {.medium}

6. Analicen las dos variables que tengan la correlaci칩n m치s alta y las que tengan la correlaci칩n m치s baja.

7. Creen el gr치fico de dispersi칩n entre el recaudo total y el recaudo en el d칤a del estreno como en la diapositiva 1. 쮼s consistente el gr치fico con el valor de la correlaci칩n?

    
    
# Regresi칩n Lineal Simple {#lm}

## 쯇or qu칠 una regresi칩n? {.medium}

### Ejemplo

El promedio en la universidad es resultado de la habilidad y las horas estudiadas. Por lo tanto, uno podr칤a pensar en el siguiente modelo:

$$Promedio=f(H,Saber11,PCA)$$
Donde $H$ es la habilidad, $Saber11$ es el puntaje en la prueba Saber 11 y $PCA$ es el porcentaje de clases a las que el estudiante fue. Esperamos que el $Promedio$ aumente cada que alguna de esas 3 variables aumente.

:::{.fragment}

Sin embargo, no necesitamos esperar. Podemos evaluar esta hip칩tesis usando un **modelo de regresi칩n!**

:::

## 쯇or qu칠 una regresi칩n? {.medium}

**Modelo de Regresi칩n:**

$$Promedio_i=\beta_0 + \beta_1H_i + \beta_2Saber11_i+\beta_3PCT_i+\varepsilon_i$$
Queremos estimar/evaluar la relaci칩n 
$$Promedio=f(H,Saber11,PCA)$$

. . .

**Preguntas a responder:**

- 쮺칩mo se interpretan $\beta_0$, $\beta_1$, $\beta_2$ y $\beta_3$?

- 쯉on los t칠rminos $\beta_k$ par치metros o son estimaciones muestrales?

- 쯈u칠 es $\varepsilon_i$?
     
## Partes esenciales de una regresi칩n {.medium}

$$y_i=\beta_0+\beta_1x_i+e_i$$

::: columns
::: {.column width="50%"}
### $y$  {.text-orange-gold .center}

- Variable dependiente

- Variable resultado

- B치sicamente lo que queremos predecir o explicar

:::

::: {.column .fragment width="50%"}

### $x$  {.text-orange-gold .center}

- Variable explicativa

- Predictor

- Variable independiente

- Lo que usamos para predecir o explicar $y$
:::
:::


## Objetivos de una regresi칩n {.medium}

Usualmente ajustamos a una l칤nea por dos razones:

### Predicci칩n

- Predecir el futuro

- Nos enfocamos en $y$

- Netflix tratando de predecir la siguiente serie que veremos

### Explicaci칩n

- Explicar el efecto de $x$ en $y$

- Nos enfocamos en $x$

- Netflix evaluando el efecto de la hora del d칤a en la selecci칩n de una serie


## 쮺칩mo se estima la l칤nea de regresi칩n? {.medium}

1. Graficar ambas variables $y$ y $x$

2. Dibujar una recta que se aproxime a la relaci칩n observada (ojal치 funcione para datos que no est치n en la muestra)

3. Estimar los n칰meros que componen esa recta

4. Interpretar esos n칰meros

```{r}
#| echo: false
library(tidyverse)
library(patchwork)
library(broom)
library(knitr)
galletas <- tibble(felicidad = c(0.5, 2, 1, 2.5, 3, 1.5, 2, 2.5, 2, 3),
                  galletas = 1:10)

galletas_datos <- galletas
modelo_galletas <- lm(felicidad ~ galletas, data = galletas_datos)
predict_galletas <- augment(modelo_galletas)
```



## Galletas y Felicidad {.medium}

::: {.tbl-classic .tbl-larger .center-text}


```{r}
#| echo: false
#| eval: true

galletas |> 
  knitr::kable(format = "html") |> 
  kableExtra::kable_styling(font_size = 30) # Adjust the font size as needed
```

:::

## 쮺칩mo se relacionan las galletas con la felicidad? {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4



base_cookies <- ggplot(predict_galletas, aes(x = galletas, y = felicidad)) +
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

base_cookies
```




## Trazando una curva que sigue los datos {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = lm, color = "#0074D9", formula = y ~ splines::bs(x, 7), se = FALSE)
```

## Otra l칤nea que se adapta a los datos {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = "loess", color = "#0074D9", se = FALSE)
```

## Ajuste con una l칤nea recta (Regresi칩n lineal) {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE)
```

## Errores de predicci칩n {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

galletas_con_residuo <- base_cookies +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE) +
  geom_segment(aes(xend = galletas, yend = .fitted), color = "#FF851B", size = 1)

galletas_con_residuo
```

## Visualizaci칩n de los errores

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

galletas_solo_residuo <- ggplot(predict_galletas, aes(x = galletas, y = .resid)) +
  geom_hline(yintercept = 0, color = "#B10DC9", size = 1) +
  geom_point(size = 3) +
  geom_segment(aes(xend = galletas, yend = 0), color = "#FF851B", size = 1) +
  coord_cartesian(xlim = c(0, 10), ylim = c(-1.5, 1.5)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Galletas consumidas", y = "Distancia a la l칤nea") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

galletas_solo_residuo
```

##


```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

(galletas_con_residuo + labs(title = "Galletas y Felicidad")) + 
  (galletas_solo_residuo + labs(title = "Errores"))
```

## Pendiente de una recta {.medium}

$$
y = mx + b
$$


- $y$ es un n칰mero

- $x$ es un n칰mero

- $m$ es la [pendiente]{.marker-hl}  $\frac{\Delta y}{\Delta x}$

- $b$ es el [intercepto]{.marker-hl} con $y$

## Pendiente de una recta: Ejemplos {.medium}

::: columns
::: {.column width="50%"}
$$
y = 2x - 1
$$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 4.8
#| fig-height: 3.5

ggplot(data = tibble(x = 0:5), aes(x = x)) +
  stat_function(fun = function(x) 2 * x - 1, 
                color = "#BF3984", size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:5) +
  scale_y_continuous(breaks = -1:9) +
  theme(panel.grid.minor = element_blank())
```

:::



::: {.column .fragment width="50%"}
$$
y = -0.5x + 6
$$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 4.8
#| fig-height: 3.5

ggplot(data = tibble(x = 0:14), aes(x = x)) +
  stat_function(fun = function(x) -0.5 * x + 6, 
                color = "#BF3984", size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:14) +
  scale_y_continuous(breaks = -1:9) +
  theme(panel.grid.minor = element_blank())
```

:::
:::

## Partes esenciales de una regresi칩n II {.medium}

$$
y = \beta_0 + \beta_1 x + \varepsilon
$$

-   $\beta_1$: la pendiente [verdadera]{.marker-hl}  de la relaci칩n entre $x$ y $y$

-   $\beta_0$: el intercepto [verdadero]{.marker-hl}  de la relaci칩n entre $x$ y $y$

-   $\varepsilon$: el error

## Partes esenciales de una regresi칩n II {.medium}

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x 
$$

- $\hat{y}$: es la estimaci칩n de la variable dependiente dado nuestro modelo

- $\hat{\beta_1}$: la pendiente [estimada]{.marker-hl}  de la relaci칩n entre $x$ y $y$

- $\hat{\beta_0}$: el intercepto [estimada]{.marker-hl}  de la relaci칩n entre $x$ y $y$

- No hay error!!!

. . .

- [El s칤mbolo *gorro* ($\hat{\beta_k}$ o $\hat{y}$) significa que es una estimaci칩n y no un par치metro real]{.hl .hl-dred}

## Modelo de Regresi칩n {.medium}

::: columns
::: {.column width="40%"}
$$  
  \begin{aligned}
y &= \color{#0074D9}{\text{Modelo}} + \color{#FF851B}{\text{Error}} \\
  &= \color{#0074D9}{f(x)} + \color{#FF851B}{\varepsilon}
\end{aligned}
$$
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| out-width: "100%"


galletas_con_residuo
```
:::
:::

## Los errores {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| out-width: "100%"


galletas_con_residuo

```

$$
\color{#FF851B}{\text{Error}} = \text{Observado} - \color{#0074D9}{\text{Predicho}} = y - \color{#0074D9}{\hat{y}}
$$

## La l칤nea de los m칤nimos cuadrados ordinarios (MCO) {.medium}

-   El error para la observaci칩n $i$ es:

$$e_i= \text{Observado} - \text{Predicho}=y_i - \hat{y_i}$$ - La **suma de los residuos al cuadrado** es:

$$e_1^2+e_2^2+e_3^2+..+e_n^2$$

-   La **l칤nea de los m칤nimos cuadrados ordinarios** es la que minimiza la suma de los residuos al cuadrado:

$$ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \sum e_i^2 $$


## 

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/meme.png")

```


## MCO vs Otras l칤neas {.smaller}

Volvamos a la relaci칩n felicidad y galletas...

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
theme_empty
```


## MCO vs Otras l칤neas {.smaller}

Para cualquier l칤nea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras l칤neas {.smaller}

Para cualquier l칤nea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras l칤neas {.smaller}

Para cualquier l칤nea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras l칤neas {.smaller}

Para cualquier l칤nea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras l칤neas {.smaller}

$\sum e_i^2$ calcula el cuadrado de los errores: errores m치s grandes reciben una penalizaci칩n mayor

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```


## MCO vs Otras l칤neas {.smaller}

La l칤nea de MCO es la combinaci칩n de $\hat{\beta_0}$ y $\hat{\beta_1}$ que minimizan $\sum e_i^2$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

## 쮺칩mo gr치ficar la l칤nea de regresi칩n? {.medium}

`geom_smooth(method="lm")`es la funci칩n dentro de ggplot para gr치ficar la l칤nea de regresi칩n y su respectivo intervalo de confianza.

::: columns
::: {.column width="40%"}

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "5" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::
:::


## 쮺칩mo gr치ficar la l칤nea de regresi칩n? {.medium}

La opci칩n `se = FALSE` elimina los intervalos de confianza.

::: columns
::: {.column width="40%"}

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "5" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE) +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "4" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE) +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::
:::




## Estimar una regresi칩n en R {.medium}

-   La funci칩n `lm()` estima una regresi칩n lineal y la funci칩n `summary()` reporta los resultados:

```{r}
#| eval: false
#| echo: true

name_of_model <- lm(y ~ x, data = DATA)

summary(name_of_model)  # Para ver los detalles del modelo
```

::: fragment

-   La funci칩n `tidy` del paquete `broom` reporta  los resultados del modelo como un data frame para graficar:

```{r}
#| eval: false
#| echo: true

library(broom)

tidy(name_of_model)
```
:::

## Modelando Galletas y Felicidad {.medium}

::: columns
::: {.column width="40%"}
$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas$$

```{r}
#| echo: true
#| eval: true
modelo_felicidad <- 
  lm(felicidad ~ galletas,
     data = galletas_datos)
```
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| label: cookies-happiness-again
#| results: hide


base_cookies +
  geom_smooth(method = "lm", color = "#0074D9") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::
:::

## Modelando Galletas y Felicidad {.medium}

La funci칩n `tidy` nos permite ver los:

  - coeficientes
  - errores est치ndar
  - el estad칤stico $t$
  - el p-value
  - los IC
  
. . .
  
Para nuestro `modelo_felicidad`:

```{r}
#| echo: true
tidy(modelo_felicidad, conf.int = TRUE)
```



## Traduciendo los resultados a matem치ticas {.medium}

::: columns
::: {.column width="40%"}
```{r}
#| echo: false
tidy(modelo_felicidad, conf.int = TRUE) |> 
  select(term, estimate)
```

$$
\begin{aligned}
&\widehat{Felicidad} = \\ 
&\hat{\beta_0}+\hat{\beta_1}\times Galletas
\end{aligned}
$$

$$
\begin{aligned}
&\widehat{Felicidad} = \\ 
&1.1 + 0.16 \times Galletas
\end{aligned}
$$
:::

::: {.column width="60%"}
```{r}
#| echo: false
base_cookies +
  geom_smooth(method = "lm", color = "#0074D9") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::
:::

## Interpretaci칩n de los coeficientes {.medium}

Un incremento en una unidad de $X$ est치 *asociado* con un incremento (o reducci칩n) promedio de $\beta_1$ unidades en $Y$

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas$$

$$\widehat{Felicidad} = 1.1 + 0.16 \times Galletas$$

:::{.incremental}

- En *promedio*, una galleta adicional est치 asociado a aumento en la felicidad de 0.16 unidades

- Si no hay consumo de galletas, esperamos que el puntaje de felicidad sea 1.1 unidades
:::


## 쮼s el intercepto importante? {.medium}

- La interpretaci칩n del intercepto es importante si en el contexto de los datos:

  1. La variable independiente puede tomar valores iguales o cercanos a cero
  
  2. La variable independiente tiene valores cercanos a cero en los datos observados
  
:::{.incremental}
  
- En caso contrario, el intercepto no tiene ninguna interpretaci칩n pr치ctica

- Veremos m치s ejemplos sobre esto m치s adelante...
:::


## 游눩 Ejercicio 2  {.medium}

Seg칰n la sabidur칤a popular en Hollywood, el recaudo durante el primer fin de semana es un fuerte predictor del 칠xito comercial de una pel칤cula.

1. Grafiquen la l칤nea de regresi칩n entre el recaudo total en Estados Unidos y el recaudo en el primer fin de semana para evaluar esta creencia.

2. Estimen la siguiente regresi칩n y ll치menla `hollywood_model`:

$$\widehat{\text{US Total Gross}} = \hat{\beta_0} + \hat{\beta_1} \times \text{Opening Gross}$$
3. Usando la funci칩n `tidy` reporten los valores de los coeficientes.

4. 쮺u치l es la interpretaci칩n de $\hat{\beta_1}$ en este caso?쯏 de $\hat{\beta_0}$?
 



## Predicci칩n  {.medium}

```{r}
#| echo: false
#| eval: true
hollywood_model <- lm(us_gross ~ opening_gross, data=hollywood)
```

$$\widehat{\text{US Total Gross}} = 5,108,220 + 3.12 \times \text{Opening Gross}$$


Seg칰n nuestro modelo, 쯖u치l ser칤a el recaudo en US de una pel칤cula cuyo recaudo en el primer fin de semana fue de \$50,000,000?



$$  
  \begin{aligned}
\widehat{\text{US Gross}} &= 5,108,220 + 3.12 \times \text{Opening Gross} \\
  &= 5,108,220 + 3.12 \times \color{red}{50,000,000} \\
  &= 161,108,220
\end{aligned}
$$

## Predicci칩n con R {.medium}

El comando `predict()` nos permite predecir $\widehat{\text{US Gross}}$ para uno o varios valores:



```{r}
#| echo: true
#| eval: true

# Creamos los valores para los cuales queremos predecir
valores_opening <- data.frame(opening_gross = c(20000000,40000000,50000000))


# Predice los valores con los coeficientes estimados
# en hollywood_model
predict(hollywood_model, newdata = valores_opening)

```


## Predicci칩n  {.medium}


```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-height: 4
#| fig-width: 8

model <- lm(us_gross ~ opening_gross , data = hollywood)

# Predict the corresponding y value when x = 50
predicted_y <- predict(model, newdata = data.frame(opening_gross = 50 * 1000000))

# Plot
ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_vline(xintercept = 50, linetype = "dashed", color = "blue") +   # Dashed vertical line at x = 50
  geom_hline(yintercept = predicted_y/ 1000000, linetype = "dashed", color = "blue") + # Dashed horizontal line at predicted y
  geom_point(aes(x = 50, y = predicted_y/ 1000000), color = "red", size = 3) + # Point at (50, predicted_y)
  labs(
    x = "Opening Gross (in millions)",
    y = "US Total Gross (in millions)"
  ) +
  theme_minimal()
```





## 쮼s posible la extrapolaci칩n? {.medium}

Extrapolar es tratar de predecir Y fuera del rango de valores de X. Es posible pero no aconsejable.

```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-height: 4
#| fig-width: 8
# Predict the corresponding y value when x = 80
predicted_y <- predict(model, newdata = data.frame(opening_gross = 80 * 1000000))

# Plot
ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
    geom_point() +
    geom_smooth(method = "lm") +
    geom_vline(xintercept = 80, linetype = "dashed", color = "blue") +   # Dashed vertical line at x = 80
    geom_hline(yintercept = predicted_y/ 1000000, linetype = "dashed", color = "blue") + # Dashed horizontal line at predicted y
    geom_point(aes(x = 80, y = predicted_y/ 1000000), color = "red", size = 3) + # Point at (80, predicted_y)
    labs(
        x = "Opening Gross (in millions)",
        y = "US Total Gross (in millions)"
    ) + theme_minimal()

```


## Inferencia de los coeficientes {.medium}

Cuando trabajamos con distribuciones muestrales, la idea era que:

$$
\bar{X} \xrightarrow{\text{游 ojal치 游룧} \mu
$$

:::{.fragment}

De igual manera, en el modelo de regresi칩n queremos:

$$
\hat{\beta} \xrightarrow{\text{游 ojal치 游룧} \beta
$$
:::



## Inferencia de los coeficientes {.medium}

$$\widehat{\text{US Total Gross}} = 5,108,220 + 3.12 \times \text{Opening Gross}$$

- Es $\beta_1$ diferente de cero?


::: fragment
```{r}
#| echo: true
tidy(hollywood_model, conf.int = TRUE)
```
:::


## M치s pruebas de hip칩tesis {.medium}

$$H_0:\beta_1=0$$
$$H_1: \beta_1 \neq 0$$

:::{.fragment}

$$t=\dfrac{3.12-0}{0.218}=14.3>t_{\alpha=0.05}=1.96$$

- Rechazamos la $H_0$ a un nivel de significancia del 5%! 

- El p-value es menor a 0.05:

```{r}
#| echo: true
tidy(hollywood_model, conf.int = TRUE)[2,5]<0.05
```

:::


# Regresi칩n Lineal M칰ltiple {#multiple}

## Regresi칩n M칰ltiple {.medium}

No estamos limitados a una sola variable explicativa!

$$
\hat{y} = \hat{\beta_0}  + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_n} x_n 
$$

:::{.fragment}
Podr칤amos pensar que el verdadero modelo de felicidad es:

  $$\widehat{\text{Felicidad}}=\hat{\beta_0}+\hat{\beta_1}\times \text{Galletas}+\hat{\beta_2} \times \text{Estudiante}$$

O que para predecir el recaudo de una pel칤cula se necesita el siguiente modelo:

$$
\widehat{\text{US Gross}} = \hat{\beta_0} + \hat{\beta_1} \text{Opening Gross} + \hat{\beta_2} \text{Budget} + \hat{\beta_3} \text{Sequel}
$$

:::



## 쮺칩mo pensar visualmente la relaci칩n $y=f(x_1,x_2)+\varepsilon$? {.medium}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/dots.png")

```


## 쮺칩mo pensar visualmente la "recta" en una regresi칩n m칰ltiple? {.medium}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/plane.png")

```



## Variables Categ칩ricas vs Variables Continuas {.medium}

En los dos modelos antes explicados hay dos tipos de variables:

### Variables Categ칩ricas

- $\text{Estudiante}$

- $\text{Sequel}$

### Variables Continuas

- $\text{Galletas}$

- $\text{Opening Gross}$



## Variables Categ칩ricas vs Variables Continuas {.medium}

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/slider-switch-plain-80.jpg")

```


## Variables Categ칩ricas vs Variables Continuas {.medium}

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/slider-switch-annotated-80.jpg")

```


## Ejemplo de felicidad y galletas

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

set.seed(123)

# Create a tibble with the same slope but different intercepts for professors and students, adding noise
cookies_data <- tibble(
  cookies = rep(1:7, 2),  # Galletas consumidas (1 through 7 for both groups)
  group = rep(c("Profesores", "Estudiantes"), each = 7),  # Group variable
  happiness = c(0.5 + 0.15 * (1:7) + rnorm(7, sd = 0.2),  # Happiness for professors with noise
                1.5 + 0.15 * (1:7) + rnorm(7, sd = 0.2))  # Happiness for students with noise
)


ggplot(cookies_data, aes(x = cookies, y = happiness)) +
  geom_point( size = 3) +  # Points still colored by group for clarity
  geom_smooth(method = "lm", se = FALSE, color = "#0074D9") +  # Single regression line for the entire dataset
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())


```


## 쯈u칠 pasa si diferenciamos entre profesores y estudiantes?

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

ggplot(cookies_data, aes(x = cookies, y = happiness)) +
  geom_point(aes(color = group), size = 3) +  # Points still colored by group for clarity
  geom_smooth(method = "lm", se = FALSE, color = "#0074D9") +  # Single regression line for the entire dataset
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad", color = "Grupo") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_color_manual(values = c("Profesores" = "navy", "Estudiantes" = "darkred")) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())


```

## Ambos grupos parecen tener pendientes diferentes...


```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

ggplot(cookies_data, aes(x = cookies, y = happiness, color = group)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE) +  # Add linear regression lines
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad", color = "Grupo") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_color_manual(values = c("Profesores" = "navy", "Estudiantes" = "darkred")) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())

```


## Variables Categ칩ricas {.medium}

쮺칩mo interpretar el modelo si agregamos una variable categ칩rica $Estudiante$ que sea igual a 1 si la observaci칩n es de un estudiante?

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas+\hat{\beta_2} \times Estudiante$$

:::{.fragment}

- El intercepto para las observaciones de los profesores ser치 $\hat{\beta_0}$ porque $Estudiante=0$

$$\widehat{Felicidad}=\color{#FF851B}{\hat{\beta_0}}+\hat{\beta_1}\times Galletas$$

:::


## Variables Categ칩ricas {.medium}

쮺칩mo interpretar el modelo si agregamos una variable categ칩rica $Estudiante$ que sea igual a 1 si la observaci칩n es de un estudiante?

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas+\hat{\beta_2} \times Estudiante$$


- El intercepto para las observaciones de los estudiantes ser치 $\hat{\beta_0}+\hat{\beta_2}$ porque $Estudiante=1$

$$\widehat{Felicidad}=\color{#FF851B}{\hat{\beta_0}}+\hat{\beta_1}\times Galletas+\color{#FF851B}{\hat{\beta_2}}$$


## Estimar una regresi칩n m칰ltiple en R {.medium}

-   Nuevamente usamos la funci칩n `lm()`, simplemente separamos las variables independientes con un `+`:

```{r}
#| eval: false
#| echo: true

name_of_model <- lm(y ~ x1 + x2 + x3, data = DATA)

tidy(name_of_model)  
```


## 游눩 Ejercicio 3  {.medium}

1. Estimen la siguiente regresi칩n con el nombre `hollywood_model_2`:

$$
\widehat{\text{US Gross}} = \hat{\beta_0} + \hat{\beta_1} \text{Opening Gross} + \hat{\beta_2} \text{Budget} + \hat{\beta_3} \text{Sequel}
$$

2. Usando la funci칩n `tidy` respondan: 쯖u치les coeficientes son estad칤sticamente diferentes de cero?

3. Completen el siguiente c칩digo para predecir el recaudo total para una secuela con recaudo del fin de semana promedio y presupuesto promedio:

```{r}
#| echo: true
#| eval: false
multiples_valores <- data.frame(opening_gross=_________,
                             budget=__________,
                             sequel=__)
predict(hollywood_model_2, newdata = multiples_valores)

```


## Predicci칩n en Regresi칩n M칰ltiple {.medium}

```{r}
#| echo: false
hollywood_model <- lm(us_gross ~ opening_gross + budget + sequel, data=hollywood)
```

As칤 como en el caso con una variable, usamos el comando `predict()` para predecir $\widehat{\text{US Gross}}$. En este caso, necesitamos al menos un valor para cada variable que est치 en la regresi칩n.


Hagamos una predicci칩n para una de las observaciones en nuestros datos. En este caso, para la pel칤cula "The Holiday":

```{r}
#| echo: true
#| eval: true
# Seleccionemos las 3 variables dependientes para The Holiday
the_holiday <- hollywood |> 
  filter(movie=="The Holiday") |>
  select(opening_gross, budget, sequel)

# Usamos el comando predict nuevamente
y_gorro <- predict(hollywood_model, newdata = the_holiday)
y_gorro
```


## Predicci칩n en Regresi칩n M칰ltiple {.medium}


- 쮼s precisa nuestra estimaci칩n? Comparemos nuestro estimado con el recaudo real de la pel칤cula:

```{r}
#| echo: true
#| eval: true
# El valor observado
y <- hollywood |>
  filter(movie == "The Holiday") |>
  pull(us_gross)
y
```

. . .

- El error para "The Holiday" ser치:

$$e=y - \hat{y}$$

```{r}
#| echo: true
#| eval: true
y - y_gorro
```




## Filtrar la variaci칩n {.medium}

- Cada **$x$** en el modelo explica una porci칩n de la variaci칩n en **$y$**

- La interpretaci칩n ac치 es m치s complicada que en el modelo de regresi칩n simple porque s칩lo se puede mover una variable a la vez


## Interpretaci칩n para variables continuas {.medium}

Manteniendo todo lo dem치s constante, un incremento de una unidad en **$x$** est치 asociado con un incremento/reducci칩n promedio de $\beta_k$ en **$y$**

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel}
\end{aligned}
$$

:::{.fragment}
Manteniendo todo lo dem치s constante, un incremento de un d칩lar en el recaudo del primer fin de semana est치 asociado con un incremento promedio de 2.99 d칩lares en el recaudo total en US
:::


## Interpretaci칩n para variables categ칩ricas {.medium}

Manteniendo todo lo dem치s constante, **$y$** es, en promedio, $\beta_k$ unidades mayor/menor para **$x$**<sub>k</sub> comparado con **$x$**<sub>omitida</sub>

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel}
\end{aligned}
$$


:::{.fragment}

Manteniendo todo lo dem치s constante, las sequelas est치n asociadas a un recaudo promedio menor, en aproximadamente $11.9 millones, comparadas con las pel칤culas que no son secuelas

:::


## Variable categ칩ricas con m치s de 2 niveles {.medium}

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel} - 15,000,000 \times \text{Trilogy}
\end{aligned}
$$

Si es la primera pel칤cula $Sequel=Trilogy=0$, el modelo es:

$$\widehat{\text{US Gross}} = -8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget}$$


## Variable categ칩ricas con m치s de 2 niveles {.medium}

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel} - 15,000,000 \times \text{Trilogy}
\end{aligned}
$$


Si es trilog칤a, entonces $Sequel=0$ y $Trilogy=1$. En este caso, el modelo es:

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 15,000,000 
\end{aligned}
$$

Manteniendo lo dem치s constante, estimamos que una trilog칤a tendr치, en promedio, un recaudo 15 millones de d칩lares menor que una primera entrega


