---
title: Anal铆tica de los Negocios
author: Carlos Cardona Andrade
subtitle: Intro a la Regresi贸n Lineal
execute:
  freeze: auto
  echo: true
  fig-width: 6
  fig-height: 5
format:
  revealjs: 
   theme: ../slides.scss
   header-includes: |
      <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" rel="stylesheet"/>
   slide-number: true
   show-slide-number: all
   transition: fade
   progress: true
   multiplex: false
   scrollable: false
   preview-links: false
   hide-inactive-cursor: true
   highlight-style: printing
   pause: true
---


```{r}
#| eval: true
#| echo: false
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel,gtools)

# Define pink color
red_pink <- "#e64173"

theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, 0, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)


# Set population and sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))


# Simulation

```



## Plan para hoy

1. [Correlaci贸n](#corr)

2. [Regresi贸n Lineal Simple](#lm)

3. [Regresi贸n Lineal M煤ltiple](#multiple)

# Correlaci贸n {#corr}


## US Total Gross vs Opening Gross {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

library(tidyverse)
library(readxl)
library(janitor)
hollywood <- read_excel("C:/Users/ccard/Downloads/KEL702-XLS-ENG.xls", sheet = "Exhibit 1")
hollywood <- hollywood %>%
  clean_names()
hollywood <- hollywood %>% rename(us_gross = total_u_s_gross)
hollywood <- hollywood %>% rename(non_us_gross = total_non_u_s_gross)

```

```{r}
#| echo: true
#| eval: true
#| fig-align: center
#| code-fold: true

ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
    geom_point() +
    labs(
        x = "Opening Gross (in millions)",
        y = "US Total Gross (in millions)"
    ) +
    theme_minimal()

```

## La correlaci贸n en R {.medium}

- Podemos utilizar un diagrama de dispersi贸n para realizar un primer an谩lisis de la relaci贸n entre dos variables

- El coeficiente de correlaci贸n (lineal) es utilizado para medir la fuerza de la asociaci贸n (lineal) entre dos variables

- La correlaci贸n entre el recaudo en US y el recaudo el primer fin de semana:

```{r}
#| echo: true
cor(hollywood$us_gross,hollywood$opening_gross)
```


## Funci贸n `corrplot` {.medium}

```{r}
#| echo: true
#| eval: false
#| fig-align: center

# RECUERDEN INSTALAR EL PAQUETE PRIMERO!!!
# install.packages("corrplot")

# Cargamos el paquete
library(corrplot)

# Creemos una base de datos temporal s贸lo con estas 4 variables
hollywood_sub <- hollywood %>% select( us_gross, opening_gross, non_us_gross, budget) 

# Creemos la matriz de correlaciones
corrplot(cor(hollywood_sub ),
         method = "number",
         type = "upper")

```



## Funci贸n `corrplot` {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: center

# RECUERDEN INSTALARLA PRIMERO!!!
# install.packages("corrplot)

# Cargamos el paquete
library(corrplot)

# Creemos una base de datos temporal s贸lo con estas 4 variables
hollywood_sub <- hollywood %>% select( us_gross, opening_gross, non_us_gross, budget) 

# Creemos la matriz de correlaciones
corrplot(cor(hollywood_sub ),
         method = "number",
         type = "upper")

```



## La correlaci贸n no es suficiente {.medium}


```{r}
#| echo: false
#| eval: true
#| fig-align: center

library(datasauRus)
library(patchwork)

dino <- datasaurus_dozen %>%
  filter(dataset == "dino")

star <- datasaurus_dozen %>%
  filter(dataset == "star")

hlines <- datasaurus_dozen %>%
  filter(dataset == "h_lines")

dino %>%
  summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
  )



```



```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-width: 7
#| fig-height: 3.5

ggplot(dino, aes(x=x, y=y))+
  geom_point(color = "darkred") + theme_minimal()
```




## La correlaci贸n no es suficiente {.medium}


```{r}
#| echo: false
#| eval: true
#| fig-align: center
star %>%
  summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
  )
```

```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-width: 7
#| fig-height: 3.5
ggplot(star, aes(x=x, y=y))+
  geom_point(color = "navy") + theme_minimal()

```


## La correlaci贸n no es suficiente {.medium}


```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-width: 7
#| fig-height: 3.5
hlines %>%
  summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
  )
```

```{r}
#| echo: false
#| eval: true
#| fig-align: center

ggplot(hlines, aes(x=x, y=y))+
  geom_point(color = "orange") + theme_minimal()

```



# Regresi贸n Lineal Simple {#lm}

## 驴Por qu茅 una regresi贸n? {.medium}

### Ejemplo

El promedio en la universidad es resultado de la habilidad y las horas estudiadas. Por lo tanto, uno podr铆a pensar en el siguiente modelo:

$$Promedio=f(H,Saber11,PCA)$$
Donde $H$ es la habilidad, $Saber11$ es el puntaje en la prueba Saber 11 y $PCA$ es el porcentaje de clases a las que el estudiante fue. Esperamos que el $Promedio$ aumente cada que alguna de esas 3 variables aumente.

:::{.fragment}

Sin embargo, no necesitamos esperar. Podemos evaluar esta hip贸tesis usando un **modelo de regresi贸n!**

:::

## 驴Por qu茅 una regresi贸n? {.medium}

**Modelo de Regresi贸n:**

$$Promedio_i=\beta_0 + \beta_1H_i + \beta_2Saber11_i+\beta_3PCT_i+\varepsilon_i$$
Queremos estimar/evaluar la relaci贸n 
$$Promedio=f(H,Saber11,PCA)$$

. . .

**Preguntas a responder:**

- 驴C贸mo se interpretan $\beta_0$, $\beta_1$, $\beta_2$ y $\beta_3$?

- 驴Son los t茅rminos $\beta_k$ par谩metros o son estimaciones muestrales?

- 驴Qu茅 es $\varepsilon_i$?
     
## Partes esenciales de una regresi贸n {.medium}

$$y_i=\beta_0+\beta_1x_i+e_i$$

::: columns
::: {.column width="50%"}
### $y$  {.text-orange-gold .center}

- Variable dependiente

- Variable resultado

- B谩sicamente lo que queremos predecir o explicar

:::

::: {.column .fragment width="50%"}

### $x$  {.text-orange-gold .center}

- Variable explicativa

- Predictor

- Variable independiente

- Lo que usamos para predecir o explicar Y
:::
:::


## Objetivos de una regresi贸n {.medium}

Usualmente ajustamos a una l铆nea por dos razones:

### Predicci贸n

- Predecir el futuro

- Nos enfocamos en Y

- Netflix tratando de predecir la siguiente serie que veremos

### Explicaci贸n

- Explicar el efecto de X en Y

- Nos enfocamos en X

- Netflix evaluando el efecto de la hora del d铆a en la selecci贸n de una serie


## 驴C贸mo se estima la l铆nea de regresi贸n? {.medium}

1. Graficar ambas variables $y$ y $x$

2. Dibujar una recta que se aproxime a la relaci贸n observada (ojal谩 funcione para datos que no est谩n en la muestra)

3. Estimar los n煤meros que componen esa recta

4. Interpretar esos n煤meros

```{r}
#| echo: false
library(tidyverse)
library(patchwork)
library(broom)
library(knitr)
galletas <- tibble(felicidad = c(0.5, 2, 1, 2.5, 3, 1.5, 2, 2.5, 2, 3),
                  galletas = 1:10)

galletas_datos <- galletas
modelo_galletas <- lm(felicidad ~ galletas, data = galletas_datos)
predict_galletas <- augment(modelo_galletas)
```



## Galletas y Felicidad {.medium}

::: {.tbl-classic .tbl-larger .center-text}


```{r}
#| echo: false
#| eval: true

galletas |> 
  knitr::kable(format = "html") |> 
  kableExtra::kable_styling(font_size = 30) # Adjust the font size as needed
```

:::

## 驴C贸mo se relacionan las galletas con la felicidad? {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4



base_cookies <- ggplot(predict_galletas, aes(x = galletas, y = felicidad)) +
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

base_cookies
```




## Trazando una curva que sigue los datos {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = lm, color = "#0074D9", formula = y ~ splines::bs(x, 7), se = FALSE)
```

## Otra l铆nea que se adapta a los datos {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = "loess", color = "#0074D9", se = FALSE)
```

## Ajuste con una l铆nea recta (Regresi贸n lineal) {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

base_cookies +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE)
```

## Errores de predicci贸n {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

galletas_con_residuo <- base_cookies +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE) +
  geom_segment(aes(xend = galletas, yend = .fitted), color = "#FF851B", size = 1)

galletas_con_residuo
```

## Visualizaci贸n de los residuos

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

galletas_solo_residuo <- ggplot(predict_galletas, aes(x = galletas, y = .resid)) +
  geom_hline(yintercept = 0, color = "#B10DC9", size = 1) +
  geom_point(size = 3) +
  geom_segment(aes(xend = galletas, yend = 0), color = "#FF851B", size = 1) +
  coord_cartesian(xlim = c(0, 10), ylim = c(-1.5, 1.5)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Galletas consumidas", y = "Distancia a la l铆nea") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

galletas_solo_residuo
```

##


```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

(galletas_con_residuo + labs(title = "Galletas y Felicidad")) + 
  (galletas_solo_residuo + labs(title = "Residuos"))
```

## Pendiente de una recta {.medium}

$$
y = mx + b
$$

::: {.tbl-classic .tbl-larger .center-text}


|     |                                          |
|:---:|:----------------------------------------:|
| $y$ |                Un n煤mero                 |
| $x$ |                Un n煤mero                 |
| $m$ | La pendiente $\frac{\Delta y}{\Delta x}$ |
| $b$ |          El intercepto con $y$           |

:::

## Pendiente de una recta {.medium}

::: columns
::: {.column width="50%"}
$$
y = 2x - 1
$$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 4.8
#| fig-height: 3.5

ggplot(data = tibble(x = 0:5), aes(x = x)) +
  stat_function(fun = function(x) 2 * x - 1, 
                color = "#BF3984", size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:5) +
  scale_y_continuous(breaks = -1:9) +
  theme(panel.grid.minor = element_blank())
```

:::



::: {.column .fragment width="50%"}
$$
y = -0.5x + 6
$$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 4.8
#| fig-height: 3.5

ggplot(data = tibble(x = 0:14), aes(x = x)) +
  stat_function(fun = function(x) -0.5 * x + 6, 
                color = "#BF3984", size = 1.5) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = 0:14) +
  scale_y_continuous(breaks = -1:9) +
  theme(panel.grid.minor = element_blank())
```

:::
:::

## Regresi贸n lineal simple {.medium}

$$
y = \beta_0 + \beta_1 x + \varepsilon
$$

-   $\beta_1$: la pendiente verdadera de la relaci贸n entre $x$ y $y$

-   $\beta_0$: el intercepto verdadero de la relaci贸n entre $x$ y $y$

-   $\varepsilon$: el error

## Regresi贸n lineal simple {.medium}

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x 
$$

-   $\hat{\beta_1}$: la pendiente estimada de la relaci贸n entre $x$ y $y$

-   $\hat{\beta_0}$: el intercepto estimada de la relaci贸n entre $x$ y $y$

-   No hay error!!!

## Modelo de Regresi贸n {.medium}

::: columns
::: {.column width="40%"}
$$  
  \begin{aligned}
Y &= \color{#0074D9}{\text{Modelo}} + \color{#FF851B}{\text{Error}} \\
  &= \color{#0074D9}{f(X)} + \color{#FF851B}{\varepsilon}
\end{aligned}
$$
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| out-width: "100%"


galletas_con_residuo
```
:::
:::

## Residuos {.medium}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| out-width: "100%"


galletas_con_residuo

```

$$
\text{Residuo} = \text{Observado} - \text{Predicho} = y - \hat{y}
$$

## La l铆nea de los m铆nimos cuadrados ordinarios (MCO) {.medium}

-   El residuo para la observaci贸n $i^{th}$ es:

$$e_i= \text{Observado} - \text{Predicho}=y_i - \hat{y_i}$$ - La **suma de los residuos al cuadrado** es:

$$e_1^2+e_2^2+e_3^2+..+e_n^2$$

-   La **l铆nea de los m铆nimos cuadrados ordinarios** es la que minimiza la suma de los residuos al cuadrado:

$$ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \sum e_i^2 $$


## 

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/meme.png")

```


## MCO vs Otras l铆neas {.smaller}

Volvamos a la relaci贸n felicidad y galletas...

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
theme_empty
```


## MCO vs Otras l铆neas {.smaller}

Para cualquier l铆nea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras l铆neas {.smaller}

Para cualquier l铆nea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras l铆neas {.smaller}

Para cualquier l铆nea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras l铆neas {.smaller}

Para cualquier l铆nea $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$, podemos calcular $e_i=y_i-\hat{y_i}$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```


## MCO vs Otras l铆neas {.smaller}

$\sum e_i^2$ calcula el cuadrado de los errores: errores m谩s grandes reciben una penalizaci贸n mayor

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```


## MCO vs Otras l铆neas {.smaller}

La l铆nea de MCO es la combinaci贸n de $\hat{\beta_0}$ y $\hat{\beta_1}$ que minimizan $\sum e_i^2$

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

## 驴C贸mo gr谩ficar la l铆nea de regresi贸n? {.medium}

`geom_smooth(method="lm")`es la funci贸n dentro de ggplot para gr谩ficar la l铆nea de regresi贸n y su respectivo intervalo de confianza.

::: columns
::: {.column width="40%"}

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "5" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::
:::


## 驴C贸mo gr谩ficar la l铆nea de regresi贸n? {.medium}

La opci贸n `se = FALSE` elimina los intervalos de confianza.

::: columns
::: {.column width="40%"}

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "5" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE) +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::

::: {.column width="60%"}

```{r}
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| code-line-numbers: "4" 
ggplot(galletas_datos, 
       aes(x = galletas, 
           y = felicidad)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE) +
  labs(x = "Galletas consumidas",
       y = "Nivel de Felicidad") +
  theme_minimal()

```

:::
:::




## Construyendo modelos en R {.medium}

-   El  siguiente c贸digo estima y reporta los resultados de la regresi贸n:

```{r}
#| eval: false
#| echo: true

name_of_model <- lm(y ~ x, data = DATA)

summary(name_of_model)  # Para ver los detalles del modelo
```

::: fragment

-   La funci贸n `tidy` del paquete `broom` reporta  los resultados del modelo como un data frame para graficar:

```{r}
#| eval: false
#| echo: true

library(broom)

tidy(name_of_model)
```
:::

## Modelando Galletas y Felicidad {.medium}

::: columns
::: {.column width="40%"}
$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas$$

```{r}
#| echo: true
#| eval: true
modelo_felicidad <- 
  lm(felicidad ~ galletas,
     data = galletas_datos)
```
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| label: cookies-happiness-again
#| results: hide


base_cookies +
  geom_smooth(method = "lm", color = "#0074D9") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::
:::

## Modelando Galletas y Felicidad {.medium}

La funci贸n `tidy` nos permite ver los:

  - coeficientes
  - errores est谩ndar
  - el estad铆stico $t$
  - el p-value
  - los IC
  
. . .
  
Para nuestro `modelo_felicidad`:

```{r}
#| echo: true
tidy(modelo_felicidad, conf.int = TRUE)
```



## Traduciendo los resultados a matem谩ticas {.medium}

::: columns
::: {.column width="40%"}
```{r}
#| echo: false
tidy(modelo_felicidad, conf.int = TRUE) |> 
  select(term, estimate)
```

$$
\begin{aligned}
&\widehat{Felicidad} = \\ 
&\hat{\beta_0}+\hat{\beta_1}\times Galletas
\end{aligned}
$$

$$
\begin{aligned}
&\widehat{Felicidad} = \\ 
&1.1 + 0.16 \times Galletas
\end{aligned}
$$
:::

::: {.column width="60%"}
```{r}
#| echo: false
base_cookies +
  geom_smooth(method = "lm", color = "#0074D9") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 4))
```
:::
:::

## Interpretaci贸n de los coeficientes {.medium}

Un incremento en una unidad de $X$ est谩 *asociado* con un incremento (o reducci贸n) promedio de $\beta_1$ unidades en $Y$

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas$$

$$\widehat{Felicidad} = 1.1 + 0.16 \times Galletas$$

:::{.incremental}

- En *promedio*, una galleta adicional est谩 asociado a aumento en la felicidad de 0.16 unidades

- Si no hay consumo de galletas, esperamos que el puntaje de felicidad sea 1.1 unidades
:::


## 驴Es el intercepto importante? {.medium}

- La interpretaci贸n del intercepto es importante si en el contexto de los datos:

  1. La variable independiente puede tomar valores iguales o cercanos a cero
  
  2. La variable independiente tiene valores cercanos a cero en los datos observados
  
:::{.incremental}
  
- En caso contrario, el intercepto no tiene ninguna interpretaci贸n pr谩ctica

- Veremos m谩s ejemplos sobre esto m谩s adelante...
:::


##  Ejercicio 2  {.medium}

Seg煤n la sabidur铆a popular en Hollywood, el recaudo durante el primer fin de semana es un fuerte predictor del 茅xito comercial de una pel铆cula.

1. Grafiquen la l铆nea de regresi贸n entre el recaudo total en Estados Unidos y el recaudo en el primer fin de semana para evaluar esta creencia.

2. Estimen la siguiente regresi贸n y ll谩menla `hollywood_model`:

$$\widehat{\text{US Total Gross}} = \hat{\beta_0} + \hat{\beta_1} \times \text{Opening Gross}$$
3. Usando la funci贸n `tidy` reporten los valores de los coeficientes.

4. 驴Cu谩l es la interpretaci贸n de $\hat{\beta_1}$ en este caso?驴Y de $\hat{\beta_0}$?
 



## Predicci贸n  {.medium}

```{r}
#| echo: false
#| eval: true
hollywood_model <- lm(us_gross ~ opening_gross, data=hollywood)
```

$$\widehat{\text{US Total Gross}} = 5,108,220 + 3.12 \times \text{Opening Gross}$$


Seg煤n nuestro modelo, 驴cu谩l ser铆a el recaudo en US de una pel铆cula cuyo recaudo en el primer fin de semana fue de \$50,000,000?



$$  
  \begin{aligned}
\widehat{\text{US Gross}} &= 5,108,220 + 3.12 \times \text{Opening Gross} \\
  &= 5,108,220 + 3.12 \times \color{red}{50,000,000} \\
  &= 161,108,220
\end{aligned}
$$

## Predicci贸n con R {.medium}

El comando `predict()` nos permite predecir $\widehat{\text{US Gross}}$ para uno o varios valores:



```{r}
#| echo: true
#| eval: true

# Creamos los valores para los cuales queremos predecir
valores_opening <- data.frame(opening_gross = c(20000000,40000000,50000000))


# Predice los valores con los coeficientes estimados
# en hollywood_model
predict(hollywood_model, newdata = valores_opening)

```


## Predicci贸n  {.medium}


```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-height: 4
#| fig-width: 8

model <- lm(us_gross ~ opening_gross , data = hollywood)

# Predict the corresponding y value when x = 50
predicted_y <- predict(model, newdata = data.frame(opening_gross = 50 * 1000000))

# Plot
ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_vline(xintercept = 50, linetype = "dashed", color = "blue") +   # Dashed vertical line at x = 50
  geom_hline(yintercept = predicted_y/ 1000000, linetype = "dashed", color = "blue") + # Dashed horizontal line at predicted y
  geom_point(aes(x = 50, y = predicted_y/ 1000000), color = "red", size = 3) + # Point at (50, predicted_y)
  labs(
    x = "Opening Gross (in millions)",
    y = "US Total Gross (in millions)"
  ) +
  theme_minimal()
```





## 驴Es posible la extrapolaci贸n? {.medium}

Extrapolar es tratar de predecir Y fuera del rango de valores de X. Es posible pero no aconsejable.

```{r}
#| echo: false
#| eval: true
#| fig-align: center
#| fig-height: 4
#| fig-width: 8
# Predict the corresponding y value when x = 80
predicted_y <- predict(model, newdata = data.frame(opening_gross = 80 * 1000000))

# Plot
ggplot(data = hollywood, aes(x = (opening_gross / 1000000), y = (us_gross / 1000000))) +
    geom_point() +
    geom_smooth(method = "lm") +
    geom_vline(xintercept = 80, linetype = "dashed", color = "blue") +   # Dashed vertical line at x = 80
    geom_hline(yintercept = predicted_y/ 1000000, linetype = "dashed", color = "blue") + # Dashed horizontal line at predicted y
    geom_point(aes(x = 80, y = predicted_y/ 1000000), color = "red", size = 3) + # Point at (80, predicted_y)
    labs(
        x = "Opening Gross (in millions)",
        y = "US Total Gross (in millions)"
    ) + theme_minimal()

```


## Inferencia de los coeficientes {.medium}

Cuando trabajamos con distribuciones muestrales, la idea era que:

$$
\bar{X} \xrightarrow{\text{ ojal谩 }} \mu
$$

:::{.fragment}

De igual manera, en el modelo de regresi贸n queremos:

$$
\hat{\beta} \xrightarrow{\text{ ojal谩 }} \beta
$$
:::



## Inferencia de los coeficientes {.medium}

$$\widehat{\text{US Total Gross}} = 5,108,220 + 3.12 \times \text{Opening Gross}$$

- Es $\beta_1$ diferente de cero?


::: fragment
```{r}
#| echo: true
tidy(hollywood_model, conf.int = TRUE)
```
:::


## M谩s pruebas de hip贸tesis {.medium}

$$H_0:\beta_1=0$$
$$H_1: \beta_1 \neq 0$$

:::{.fragment}

$$Z=\dfrac{3.12-0}{0.218}=14.3>Z_{\frac{\alpha}{2}}=1.96$$

- Rechazamos la $H_0$ a un nivel de significancia del 5%! 

- El p-value es 7.07e-23 (en notaci贸n cient铆fica), el cual es mucho menor a 0.05. 

:::


# Regresi贸n Lineal M煤ltiple {#multiple}

## Regresi贸n M煤ltiple {.medium}

No estamos limitados a una sola variable explicativa!

$$
\hat{y} = \hat{\beta_0}  + \hat{\beta_1} x_1 + \hat{\beta_2} x_2 + \cdots + \hat{\beta_n} x_n 
$$

:::{.fragment}
Podr铆amos pensar que el verdadero modelo de felicidad es:

  $$\widehat{\text{Felicidad}}=\hat{\beta_0}+\hat{\beta_1}\times \text{Galletas}+\hat{\beta_2} \times \text{Estudiante}$$

O que para predecir el recaudo de una pel铆cula se necesita el siguiente modelo:

$$
\widehat{\text{US Gross}} = \hat{\beta_0} + \hat{\beta_1} \text{Opening Gross} + \hat{\beta_2} \text{Budget} + \hat{\beta_3} \text{Sequel}
$$

:::



## 驴C贸mo pensar la relaci贸n $y=f(x_1,x_2)+\varepsilon$? {.medium}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/dots.png")

```


## 驴C贸mo pensar la "recta" en una regresi贸n m煤ltiple? {.medium}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/plane.png")

```



## Variables Categ贸ricas vs Variables Continuas {.medium}

En los dos modelos antes explicados hay dos tipos de variables:

### Variables Categ贸ricas

- $\text{Estudiante}$

- $\text{Sequel}$

### Variables Continuas

- $\text{Galletas}$

- $\text{Opening Gross}$



## Variables Categ贸ricas vs Variables Continuas {.medium}

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/slider-switch-plain-80.jpg")

```


## Variables Categ贸ricas vs Variables Continuas {.medium}

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("img/slider-switch-annotated-80.jpg")

```


## Ejemplo de felicidad y galletas

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

set.seed(123)

# Create a tibble with the same slope but different intercepts for professors and students, adding noise
cookies_data <- tibble(
  cookies = rep(1:7, 2),  # Galletas consumidas (1 through 7 for both groups)
  group = rep(c("Profesores", "Estudiantes"), each = 7),  # Group variable
  happiness = c(0.5 + 0.15 * (1:7) + rnorm(7, sd = 0.2),  # Happiness for professors with noise
                1.5 + 0.15 * (1:7) + rnorm(7, sd = 0.2))  # Happiness for students with noise
)


ggplot(cookies_data, aes(x = cookies, y = happiness)) +
  geom_point( size = 3) +  # Points still colored by group for clarity
  geom_smooth(method = "lm", se = FALSE, color = "#0074D9") +  # Single regression line for the entire dataset
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())


```


## 驴Qu茅 pasa si diferenciamos entre profesores y estudiantes?

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

ggplot(cookies_data, aes(x = cookies, y = happiness)) +
  geom_point(aes(color = group), size = 3) +  # Points still colored by group for clarity
  geom_smooth(method = "lm", se = FALSE, color = "#0074D9") +  # Single regression line for the entire dataset
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad", color = "Grupo") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_color_manual(values = c("Profesores" = "navy", "Estudiantes" = "darkred")) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())


```

## Ambos grupos parecen tener pendientes diferentes...


```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

ggplot(cookies_data, aes(x = cookies, y = happiness, color = group)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE) +  # Add linear regression lines
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad", color = "Grupo") +
  theme_minimal(base_size = 14) +
  coord_cartesian(xlim = c(0, 8), ylim = c(0, 3)) +
  scale_color_manual(values = c("Profesores" = "navy", "Estudiantes" = "darkred")) +
  scale_x_continuous(breaks = 0:8) +
  theme(panel.grid.minor = element_blank())

```


## Variables Categ贸ricas {.medium}

驴C贸mo interpretar el modelo si agregamos una variable categ贸rica $Estudiante$ que sea igual a 1 si la observaci贸n es de un estudiante?

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas+\hat{\beta_2} \times Estudiante$$

:::{.fragment}

- EL intercepto para las observaciones de los profesores ser谩 $\hat{\beta_0}$ porque $Estudiante=0$

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas$$

:::


## Variables Categ贸ricas {.medium}

驴C贸mo interpretar el modelo si agregamos una variable categ贸rica $Estudiante$ que sea igual a 1 si la observaci贸n es de un estudiante?

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas+\hat{\beta_2} \times Estudiante$$


- El intercepto para las observaciones de los estudiantes ser谩 $\hat{\beta_0}+\hat{\beta_2}$ porque $Estudiante=1$

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas+\hat{\beta_2}$$


##  Ejercicio 3  {.medium}

1. Estimen la siguiente regresi贸n con el nombre `hollywood_model_2`:

$$
\widehat{\text{US Gross}} = \hat{\beta_0} + \hat{\beta_1} \text{Opening Gross} + \hat{\beta_2} \text{Budget} + \hat{\beta_3} \text{Sequel}
$$

2. Usando la funci贸n `tidy` respondan: 驴cu谩les coeficientes son estad铆sticamente diferentes de cero?

3. Completen el siguiente c贸digo para predecir el recaudo total para una secuela con recaudo del fin de semana promedio y presupuesto promedio:

```{r}
#| echo: true
#| eval: false
multiples_valores <- data.frame(opening_gross=_________,
                             budget=__________,
                             sequel=__)
predict(hollywood_model_2, newdata = multiples_valores)

```


## Predicci贸n en Regresi贸n M煤ltiple {.medium}

```{r}
#| echo: false
hollywood_model <- lm(us_gross ~ opening_gross + budget + sequel, data=hollywood)
```

As铆 como en el caso con una variable, usamos el comando `predict()` para predecir $\widehat{\text{US Gross}}$. En este caso, necesitamos al menos un valor para cada variable que est谩 en la regresi贸n.


Hagamos una predicci贸n para una de las observaciones en nuestros datos. En este caso, para la pel铆cula "The Holiday".

```{r}
#| echo: true
#| eval: true
# Seleccionemos las 3 variables dependientes para The Holiday
the_holiday <- hollywood |> 
  filter(movie=="The Holiday") |>
  select(opening_gross, budget, sequel)

# Usamos el comando predict nuevamente
predicho <- predict(hollywood_model, newdata = the_holiday)
predicho
```


## Predicci贸n en Regresi贸n M煤ltiple {.medium}


- 驴Es precisa nuestra estimaci贸n?

```{r}
#| echo: true
#| eval: true
# El valor observado
observado <- hollywood |>
  filter(movie == "The Holiday") |>
  pull(us_gross)
observado
```

- El residuo para "The Holiday" ser谩:

```{r}
#| echo: true
#| eval: true
observado-predicho
```




## Filtrar la variaci贸n {.medium}

- Cada **X** en el modelo explica una porci贸n de la variaci贸n en **Y**

- La interpretaci贸n ac谩 es m谩s complicada que en el modelo de regresi贸n simple porque s贸lo se puede mover una variable a la vez


## Interpretaci贸n para variables continuas {.medium}

Manteniendo todo lo dem谩s constante, un incremento de una unidad en **X** est谩 asociado con un incremento/reducci贸n promedio de $\beta_n$ en **Y**

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel}
\end{aligned}
$$

:::{.fragment}
Manteniendo todo lo dem谩s constante, un incremento de un d贸lar en el recaudo del primer fin de semana est谩 asociado con un incremento promedio de 2.99 d贸lares en el recaudo total en US
:::


## Interpretaci贸n para variables categ贸ricas {.medium}

Manteniendo todo lo dem谩s constante, **Y** es, en promedio, $\beta_n$ unidades mayor/menor para **X**<sub>n</sub> comparado con **X**<sub>omitida</sub>

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel}
\end{aligned}
$$


:::{.fragment}

Manteniendo todo lo dem谩s constante, las sequelas est谩n asociadas a un recaudo promedio menor, en aproximadamente $11.9 millones, comparadas con las pel铆culas que no son secuelas

:::


## Variable categ贸ricas con m谩s de 2 niveles {.medium}

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel} - 15,000,000 \times \text{Trilogy}
\end{aligned}
$$

Si es la primera pel铆cula $Sequel=Trilogy=0$, el modelo es:

$$\widehat{\text{US Gross}} = -8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget}$$


## Variable categ贸ricas con m谩s de 2 niveles {.medium}

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 11,929,834 \times \text{Sequel} - 15,000,000 \times \text{Trilogy}
\end{aligned}
$$


Si es trilog铆a, entonces $Sequel=0$ y $Trilogy=1$. En este caso, el modelo es:

$$  
  \begin{aligned}
\widehat{\text{US Gross}} = &-8,785,254 + 2.99 \times \text{Opening Gross} + 0.356 \times \text{Budget} \\
  & - 15,000,000 
\end{aligned}
$$

Manteniendo lo dem谩s constante, estimamos que una trilog铆a tendr谩, en promedio, un recaudo 15 millones de d贸lares menor que una primera entrega


## 驴Qu茅 tan bueno es el modelo? {.medium}

### R-Squared

El $R^2$ es el porcentaje de la varianza de la variable dependiente explicada por el modelo de regresi贸n

$$R^2=Corr(x,y)^2=Corr(y,\hat{y})$$

- Est谩 entre 0 (nuestro modelo no predice nada) y 1 (predicci贸n perfecta)

- No tiene unidad de medida



## 驴Qu茅 tan bueno es el modelo? {.medium}

Con la funci贸n `glance()` podemos ver diferentes aspectos que eval煤an el modelo:

```{r}
#| echo: true
glance(hollywood_model)
```

:::{.fragment}
Este modelo de regresi贸n explica el 79% de la varianza del recaudo total en US

```{r}
#| echo: true
hollywood_model <- lm(us_gross ~ opening_gross + budget + sequel, data=hollywood)
```
:::



